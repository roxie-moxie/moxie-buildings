---
phase: 06-fix-data-pipeline-bugs
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/moxie/api/routers/units.py
  - src/moxie/scheduler/runner.py
  - tests/api/test_units.py
  - tests/test_runner_failure.py
autonomous: true
requirements:
  - AGENT-01
  - INFRA-03

must_haves:
  truths:
    - "Units scraped as 'Available Now' (stored as today's YYYY-MM-DD) are returned by the API when filtering with available_before"
    - "A scraper failure in the batch runner retains existing units in the DB (does not delete them)"
    - "A scraper failure in the batch runner marks the building as stale (last_scrape_status='failed')"
    - "Both entry points (batch runner and save_scrape_result) produce identical DB state on failure: units retained, building marked failed"
  artifacts:
    - path: "src/moxie/api/routers/units.py"
      provides: "Corrected available_before filter using simple <= comparison"
      contains: "Unit.availability_date <= available_before"
    - path: "src/moxie/scheduler/runner.py"
      provides: "Unified failure handler delegating to save_scrape_result"
      contains: "save_scrape_result"
    - path: "tests/api/test_units.py"
      provides: "Corrected regression test seeding today's date instead of literal 'Available Now'"
      contains: "date.today"
    - path: "tests/test_runner_failure.py"
      provides: "Regression tests proving runner retains units and marks building stale on failure"
      contains: "test_units_retained_on_scraper_exception"
  key_links:
    - from: "src/moxie/scheduler/runner.py"
      to: "src/moxie/scrapers/base.py"
      via: "save_scrape_result() call in except block"
      pattern: "save_scrape_result\\("
    - from: "src/moxie/api/routers/units.py"
      to: "moxie.normalizer"
      via: "Filter condition matches normalizer's date output format"
      pattern: "availability_date <= available_before"
---

<objective>
Fix two integration bugs found by the v1.0 audit: the "Available Now" API filter mismatch (AGENT-01) and the dual failure-handling divergence between batch runner and save_scrape_result (INFRA-03).

Purpose: The data pipeline must behave correctly and consistently. "Available Now" units must not be silently excluded from date-filtered searches, and a scraper failure must produce the same retain-and-stale DB state regardless of entry point.

Output: Corrected filter in units.py, unified failure handler in runner.py, updated regression test in test_units.py, new regression test in test_runner_failure.py.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-fix-data-pipeline-bugs/06-RESEARCH.md
@src/moxie/api/routers/units.py
@src/moxie/scheduler/runner.py
@src/moxie/scrapers/base.py
@tests/api/test_units.py
@tests/api/conftest.py
@tests/test_save_scrape_result.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Fix "Available Now" filter and unify runner failure handler</name>
  <files>
    src/moxie/api/routers/units.py
    src/moxie/scheduler/runner.py
  </files>
  <action>
**Bug 1 — units.py (AGENT-01):**

In `search_units()`, replace the broken `available_before` filter block (lines 70-78) with a simple `<=` comparison. The normalizer converts "Available Now" to today's YYYY-MM-DD before storing, so the `Unit.availability_date == "Available Now"` branch is dead code. The fix:

Replace:
```python
if available_before is not None:
    # Include units available on or before the date AND 'Available Now' units
    from sqlalchemy import or_
    query = query.filter(
        or_(
            Unit.availability_date == "Available Now",
            Unit.availability_date <= available_before,
        )
    )
```

With:
```python
if available_before is not None:
    # "Available Now" units are stored as today's YYYY-MM-DD by the normalizer,
    # so a simple <= comparison includes them for any same-day or future cutoff.
    query = query.filter(Unit.availability_date <= available_before)
```

Remove the now-unused `from sqlalchemy import or_` import inside the function. Check if `or_` is imported elsewhere in the file — if this was its only use, there is nothing else to remove (it was a local import inside the `if` block).

**Bug 2 — runner.py (INFRA-03):**

1. Add import at top of runner.py: `from moxie.scrapers.base import save_scrape_result`

2. Replace the `except Exception` block (lines 97-120) that manually deletes units with a call to `save_scrape_result(scrape_succeeded=False)`. Replace:

```python
except Exception as e:
    db.rollback()
    error_msg = f"[{type(e).__name__}] {str(e)[:500]}"
    result["error"] = error_msg

    # Clear-on-failure: delete units (user decision — stale data is NOT real data)
    try:
        building = db.get(Building, building_id)
        if building:
            db.query(Unit).filter(Unit.building_id == building.id).delete()
            building.last_scrape_status = "failed"
            building.last_scraped_at = now
            db.add(ScrapeRun(
                building_id=building.id,
                run_at=now,
                status="failed",
                unit_count=0,
                error_message=error_msg[:1000],
            ))
            db.commit()
    except Exception:
        logger.error(f"Failed to record failure for {building_name}: {e}")
```

With:
```python
except Exception as e:
    db.rollback()
    error_msg = f"[{type(e).__name__}] {str(e)[:500]}"
    result["error"] = error_msg

    # Retain units on failure, mark building stale — delegates to save_scrape_result()
    try:
        building = db.get(Building, building_id)
        if building:
            save_scrape_result(
                db,
                building,
                raw_units=[],
                scrape_succeeded=False,
                error_message=error_msg[:1000],
            )
    except Exception:
        logger.error(f"Failed to record failure for {building_name}: {e}")
```

Note: pass `raw_units=[]` explicitly — the variable may not be defined when exception occurs before `mod.scrape()` returns. The failure path in `save_scrape_result` ignores raw_units.

Do NOT touch the success path (lines 58-95) in runner.py — that is tech debt for Phase 7, not a bug.
Do NOT change the normalizer — the normalizer is correct.
  </action>
  <verify>
Run: `cd "C:/Users/eimil/projects/Roxie Projects/moxie-buildings" && export PATH="/c/Users/eimil/.local/bin:$PATH" && uv run --no-sync python -c "from moxie.api.routers.units import router; print('units.py imports OK')"` — no ImportError.

Run: `cd "C:/Users/eimil/projects/Roxie Projects/moxie-buildings" && export PATH="/c/Users/eimil/.local/bin:$PATH" && uv run --no-sync python -c "from moxie.scheduler.runner import scrape_one_building; print('runner.py imports OK')"` — no ImportError.

Visually confirm: `units.py` has no `or_` import and no `"Available Now"` string comparison. `runner.py` except block calls `save_scrape_result()` and does NOT call `db.query(Unit).filter(...).delete()`.
  </verify>
  <done>
units.py available_before filter uses simple `<=` comparison (no dead "Available Now" branch). runner.py exception handler delegates to save_scrape_result(scrape_succeeded=False) instead of manually deleting units. Both files import cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: Fix existing regression test and add runner failure tests</name>
  <files>
    tests/api/test_units.py
    tests/test_runner_failure.py
  </files>
  <action>
**Fix test_units.py (Bug 1 regression):**

The existing `test_available_now_included_with_date_filter` test (line 166) seeds `availability_date="Available Now"` directly into the DB, bypassing the normalizer. After the Bug 1 fix, the filter `<= available_before` will not match the literal string "Available Now" because "A..." sorts before "2..." lexicographically. The test seed must use today's YYYY-MM-DD to match what the normalizer actually produces.

Replace the `test_available_now_included_with_date_filter` method with:

```python
def test_available_now_included_with_date_filter(self, client, agent_headers, db_session):
    from datetime import date
    today = date.today().strftime("%Y-%m-%d")
    seed_building_with_units(db_session, "Test Building", "River North", [
        {"unit_number": "101", "bed_type": "1BR", "rent_cents": 200000,
         "availability_date": today},          # normalized form of "Available Now"
        {"unit_number": "102", "bed_type": "1BR", "rent_cents": 200000,
         "availability_date": "2026-04-01"},   # future unit, excluded by cutoff
    ])
    # available_before=2026-03-01 should include the "Available Now" unit (today <= 2026-03-01)
    # but not the April unit
    resp = client.get("/units", params={"available_before": "2026-03-01"}, headers=agent_headers)
    assert resp.status_code == 200
    data = resp.json()
    assert data["total"] == 1
    assert data["units"][0]["unit_number"] == "101"
```

Key changes: seed `availability_date=today` (not "Available Now"), assert on `unit_number` (not `availability_date` string), add `from datetime import date` import at function scope.

**Create tests/test_runner_failure.py (Bug 2 regression):**

Create a new test file with in-memory SQLite following the same pattern as `test_save_scrape_result.py`. Test that `scrape_one_building()` retains units and marks the building stale when a scraper raises an exception.

The test file must:
1. Use `in-memory SQLite` with `Base.metadata.create_all(engine)` — same pattern as `test_save_scrape_result.py`
2. Patch `SessionLocal` in `moxie.scheduler.runner` to return the test session (runner creates its own session via `SessionLocal()`)
3. Patch `importlib.import_module` to raise `RuntimeError("Network timeout")` to trigger the except block
4. Pre-seed a building with one unit before the failure
5. Assert: unit count is still 1 after failure (units retained)
6. Assert: `building.last_scrape_status == "failed"` (marked stale)
7. Assert: a `ScrapeRun` record exists with `status="failed"` and `unit_count==0`

Important mock detail: `runner.py` calls `db = SessionLocal()` to create its own session. Patch `moxie.scheduler.runner.SessionLocal` to return the test session so the test can inspect the same DB state. Use `patch("moxie.scheduler.runner.SessionLocal", return_value=session)`.

Also patch `PLATFORM_SCRAPERS` to include the test platform: `patch("moxie.scheduler.runner.PLATFORM_SCRAPERS", {"sightmap": "moxie.scrapers.tier2.sightmap"})`.

Suppress the `time.sleep` in tests: `patch("moxie.scheduler.runner.time.sleep")`.

Three test methods in a `TestRunnerFailureHandling` class:
- `test_units_retained_on_scraper_exception`: pre-seed unit, trigger failure, assert unit count unchanged
- `test_building_marked_stale_on_failure`: trigger failure, assert `last_scrape_status == "failed"` and `last_scraped_at is not None`
- `test_scrape_run_logged_on_failure`: trigger failure, assert ScrapeRun row exists with `status="failed"`, `unit_count==0`, `error_message` contains "Network timeout"
  </action>
  <verify>
Run existing API tests: `cd "C:/Users/eimil/projects/Roxie Projects/moxie-buildings" && export PATH="/c/Users/eimil/.local/bin:$PATH" && uv run --no-sync pytest tests/api/test_units.py -v` — all tests pass, including the updated `test_available_now_included_with_date_filter`.

Run new runner failure tests: `cd "C:/Users/eimil/projects/Roxie Projects/moxie-buildings" && export PATH="/c/Users/eimil/.local/bin:$PATH" && uv run --no-sync pytest tests/test_runner_failure.py -v` — all 3 tests pass.

Run full test suite to check for regressions: `cd "C:/Users/eimil/projects/Roxie Projects/moxie-buildings" && export PATH="/c/Users/eimil/.local/bin:$PATH" && uv run --no-sync pytest tests/api/ tests/test_save_scrape_result.py tests/test_runner_failure.py -v`
  </verify>
  <done>
The updated `test_available_now_included_with_date_filter` seeds today's YYYY-MM-DD (matching normalizer output) and passes with the fixed filter. Three new tests in `test_runner_failure.py` prove the batch runner retains units, marks buildings stale, and logs a failed ScrapeRun on scraper exceptions. All tests pass.
  </done>
</task>

</tasks>

<verification>
1. `pytest tests/api/test_units.py -v` — all unit search tests pass, including the fixed Available Now regression test
2. `pytest tests/test_runner_failure.py -v` — all 3 runner failure tests pass (units retained, building marked stale, ScrapeRun logged)
3. `pytest tests/test_save_scrape_result.py -v` — existing 25 tests still pass (no regressions in save_scrape_result behavior)
4. Visual inspection: `units.py` contains `Unit.availability_date <= available_before` (no `or_`, no `"Available Now"`)
5. Visual inspection: `runner.py` except block calls `save_scrape_result(db, building, raw_units=[], scrape_succeeded=False, ...)` (no `.delete()`)
</verification>

<success_criteria>
- "Available Now" units (stored as today's YYYY-MM-DD) appear in API results when `available_before` filter is applied
- Batch runner failure retains existing units in DB (not deleted)
- Batch runner failure sets `last_scrape_status="failed"` on the building
- Both entry points produce identical DB state on failure by construction (both call `save_scrape_result(scrape_succeeded=False)`)
- All existing tests pass without modification (except the one test that was intentionally updated)
- 3 new regression tests prove the fix
</success_criteria>

<output>
After completion, create `.planning/phases/06-fix-data-pipeline-bugs/06-01-SUMMARY.md`
</output>
