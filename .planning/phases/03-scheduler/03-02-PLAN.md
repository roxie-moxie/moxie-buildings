---
phase: 03-scheduler
plan: 02
type: execute
wave: 2
depends_on:
  - "03-01"
files_modified:
  - src/moxie/scrape_all.py
  - src/moxie/scheduler/batch.py
  - src/moxie/scheduler/sheets_status.py
  - src/moxie/scheduler/log_config.py
  - pyproject.toml
autonomous: true
requirements:
  - INFRA-02

must_haves:
  truths:
    - "The APScheduler cron job fires at 2 AM Central daily when scrape-all runs in scheduled mode"
    - "After each batch run, a Google Sheet 'Scrape Status' tab shows per-building status (one row per building, latest only)"
    - "After each batch run, a summary row is appended or updated with date, total buildings, successes, failures, total units"
    - "Local log file rotates at 5 MB with 7 backups, capturing all batch activity"
    - "scrape_runs rows older than 30 days are pruned after each batch"
    - "Running scrape-all without --run-now enters scheduled mode (APScheduler blocks at 2 AM cron)"
  artifacts:
    - path: "src/moxie/scheduler/sheets_status.py"
      provides: "Google Sheets batch status push (Scrape Status tab + summary)"
      contains: "push_batch_status"
    - path: "src/moxie/scheduler/log_config.py"
      provides: "Rotating file handler setup for batch logging"
      contains: "configure_logging"
    - path: "src/moxie/scrape_all.py"
      provides: "CLI with --run-now vs scheduled mode (APScheduler)"
      contains: "BlockingScheduler"
  key_links:
    - from: "src/moxie/scrape_all.py"
      to: "src/moxie/scheduler/batch.py"
      via: "APScheduler cron trigger calls run_batch"
      pattern: "scheduler\\.add_job.*run_batch"
    - from: "src/moxie/scheduler/batch.py"
      to: "src/moxie/scheduler/sheets_status.py"
      via: "run_batch calls push_batch_status after scraping"
      pattern: "push_batch_status"
    - from: "src/moxie/scheduler/batch.py"
      to: "scrape_runs table"
      via: "DELETE from scrape_runs WHERE run_at < 30 days ago"
      pattern: "prune_old_runs"
---

<objective>
Add APScheduler 2 AM cron job, Google Sheets status reporting, rotating log files, and scrape_runs pruning on top of the batch runner from Plan 01.

Purpose: Completes INFRA-02 by making the batch run truly automated (fire-and-forget at 2 AM) and observable (check the Google Sheet "Scrape Status" tab in the morning for per-building results).

Output: `uv run scrape-all` enters scheduled mode (APScheduler blocks, fires at 2 AM daily). `uv run scrape-all --run-now` runs immediately. After each run, the Google Sheet has a "Scrape Status" tab with building-by-building results.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-scheduler/03-CONTEXT.md
@.planning/phases/03-scheduler/03-RESEARCH.md
@.planning/phases/03-scheduler/03-01-SUMMARY.md
@src/moxie/scrape_all.py
@src/moxie/scheduler/batch.py
@src/moxie/db/models.py
@src/moxie/sync/push_availability.py
@src/moxie/config.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Google Sheets status push + scrape_runs pruning + rotating log</name>
  <files>
    src/moxie/scheduler/sheets_status.py
    src/moxie/scheduler/log_config.py
    src/moxie/scheduler/batch.py
  </files>
  <action>
**1. Create `src/moxie/scheduler/sheets_status.py`:**
Pushes batch results to a "Scrape Status" tab in the Google Sheet. Single batch write (one `ws.update()` call) to stay within API rate limits.

```python
"""Push batch scrape status to Google Sheets 'Scrape Status' tab."""
import logging
from datetime import datetime, timezone

import gspread

from moxie.config import GOOGLE_SHEETS_ID, GOOGLE_SHEETS_KEY_PATH

logger = logging.getLogger("moxie.scheduler")


def push_batch_status(results: list[dict]) -> None:
    """
    Write batch results to a 'Scrape Status' tab in the configured Google Sheet.

    Creates the tab if it doesn't exist. Overwrites all rows each run (latest only,
    no history accumulation — per user decision).

    Tab layout:
    - Row 1: Summary — date, total buildings, successes, failures, total units
    - Row 2: blank separator
    - Row 3: Column headers
    - Row 4+: One row per building (building name, platform, status, units, last scraped, error)

    Uses a single ws.update() call for all data — counts as one API request.
    """
    if not results:
        logger.info("No results to push to Scrape Status sheet")
        return

    now = datetime.now(timezone.utc)
    successes = sum(1 for r in results if r["status"] == "success")
    failures = sum(1 for r in results if r["status"] == "failed")
    total_units = sum(r["unit_count"] for r in results)

    # Sort by building name for consistent display
    sorted_results = sorted(results, key=lambda r: r["building_name"].lower())

    # Build all rows in memory
    rows = []

    # Summary row
    rows.append([
        f"Last Run: {now.strftime('%Y-%m-%d %H:%M UTC')}",
        f"Buildings: {len(results)}",
        f"Success: {successes}",
        f"Failed: {failures}",
        f"Total Units: {total_units}",
        "",
    ])

    # Blank separator
    rows.append(["", "", "", "", "", ""])

    # Header
    rows.append(["Building", "Platform", "Status", "Units", "Last Scraped", "Error"])

    # Per-building rows
    for r in sorted_results:
        rows.append([
            r["building_name"],
            r["platform"],
            r["status"],
            r["unit_count"],
            r.get("scraped_at", ""),
            (r.get("error") or "")[:200],  # Truncate long errors
        ])

    # Push to Google Sheets
    try:
        gc = gspread.service_account(filename=GOOGLE_SHEETS_KEY_PATH)
        sh = gc.open_by_key(GOOGLE_SHEETS_ID)

        try:
            ws = sh.worksheet("Scrape Status")
        except gspread.exceptions.WorksheetNotFound:
            ws = sh.add_worksheet(
                title="Scrape Status",
                rows=max(len(rows) + 10, 500),
                cols=6,
            )

        ws.clear()
        ws.update(rows, value_input_option="RAW")  # Single API call for all rows
        logger.info(f"Pushed {len(sorted_results)} rows to 'Scrape Status' sheet tab")

    except Exception as e:
        # Sheet push failure should not crash the batch — it's monitoring, not core function
        logger.error(f"Failed to push batch status to Google Sheets: {e}")
```

**2. Create `src/moxie/scheduler/log_config.py`:**
Sets up rotating file handler for batch runs.

```python
"""Logging configuration for batch scrape runs."""
import logging
import os
from logging.handlers import RotatingFileHandler


def configure_logging(log_dir: str = "logs") -> None:
    """
    Configure moxie.scheduler logger with rotating file handler.

    Creates the log directory if it doesn't exist.
    File: logs/scrape_batch.log (5 MB per file, 7 backups = ~40 MB max)
    """
    os.makedirs(log_dir, exist_ok=True)

    handler = RotatingFileHandler(
        os.path.join(log_dir, "scrape_batch.log"),
        maxBytes=5 * 1024 * 1024,   # 5 MB
        backupCount=7,
        encoding="utf-8",
    )
    handler.setFormatter(logging.Formatter(
        "%(asctime)s %(levelname)s %(name)s: %(message)s"
    ))

    logger = logging.getLogger("moxie.scheduler")
    logger.addHandler(handler)
    logger.setLevel(logging.INFO)
```

**3. Update `src/moxie/scheduler/batch.py`:**
Add three integrations at the end of `run_batch()`:

a. **After the scraping step (after the summary log):** Call `push_batch_status(results)` to write to Google Sheets. Import from `moxie.scheduler.sheets_status`.

b. **After push_batch_status:** Call `_prune_old_runs()` to delete scrape_runs older than 30 days. Add this function:

```python
def _prune_old_runs(days: int = 30) -> int:
    """Delete scrape_runs rows older than `days` days. Returns count deleted."""
    from moxie.db.models import ScrapeRun
    cutoff = datetime.now(timezone.utc) - timedelta(days=days)
    db = SessionLocal()
    try:
        count = db.query(ScrapeRun).filter(ScrapeRun.run_at < cutoff).delete()
        db.commit()
        logger.info(f"Pruned {count} scrape_runs older than {days} days")
        return count
    except Exception as e:
        logger.error(f"Failed to prune old scrape_runs: {e}")
        return 0
    finally:
        db.close()
```

c. **Also push updated availability data to the Availability tab** by calling `push_availability(db)` (imported from `moxie.sync.push_availability`) after the status push. This ensures the Availability tab is refreshed with the latest scraped data after every batch run. Use a fresh SessionLocal for this call. Wrap in try/except so Sheets failure doesn't crash the batch.

Add `from datetime import timedelta` to the imports.

The end of `run_batch()` should look like:

```python
    # Step 4: Push status to Google Sheets
    from moxie.scheduler.sheets_status import push_batch_status
    push_batch_status(results)

    # Step 5: Push updated availability data to Google Sheets
    try:
        from moxie.sync.push_availability import push_availability
        db = SessionLocal()
        try:
            count = push_availability(db)
            logger.info(f"Pushed {count} units to Availability tab")
        finally:
            db.close()
    except Exception as e:
        logger.error(f"Failed to push availability to Sheets: {e}")

    # Step 6: Prune old scrape_runs
    _prune_old_runs()

    return results
```
  </action>
  <verify>
1. Run `uv run scrape-all --dry-run --skip-sync` — verify log file is created at `logs/scrape_batch.log`
2. Run a small batch (2-3 buildings) with `uv run scrape-all --skip-sync` and check: (a) `logs/scrape_batch.log` has entries, (b) Google Sheet now has a "Scrape Status" tab with per-building rows, (c) Availability tab is updated
3. Check scrape_runs table: `uv run python -c "from moxie.db.session import SessionLocal; from moxie.db.models import ScrapeRun; db=SessionLocal(); print(db.query(ScrapeRun).count(), 'scrape_runs')"` — should show recent entries
  </verify>
  <done>
After each batch run: Google Sheet "Scrape Status" tab shows summary row + per-building status. Local log file at logs/scrape_batch.log captures all batch activity with 5MB rotation. scrape_runs older than 30 days are pruned. Availability tab is refreshed.
  </done>
</task>

<task type="auto">
  <name>Task 2: APScheduler 2 AM cron + scheduled mode CLI</name>
  <files>
    src/moxie/scrape_all.py
    pyproject.toml
  </files>
  <action>
**1. Add APScheduler dependency:**
Run `uv add apscheduler` to add APScheduler 3.x to the project. This should add `apscheduler>=3.10` (or similar) to pyproject.toml dependencies.

**2. Update `src/moxie/scrape_all.py`:**
Restructure main() to support two modes:
- `scrape-all --run-now` (or just `scrape-all`): Run batch immediately, then exit (current behavior from Plan 01)
- `scrape-all --schedule`: Enter scheduled mode — APScheduler blocks, fires at 2 AM Central daily

The `--run-now` flag should be the default when no scheduling flags are passed (so `uv run scrape-all` still works as immediate run). Add `--schedule` flag for the daemon-like scheduled mode.

```python
"""
Batch scrape CLI entrypoint.

Usage:
    scrape-all                    # Run batch immediately, then exit
    scrape-all --run-now          # Same as above (explicit)
    scrape-all --schedule         # Enter scheduled mode (2 AM Central daily, blocks)
    scrape-all --dry-run          # List buildings without scraping
    scrape-all --skip-sync        # Skip Google Sheets sync step

Entrypoint: moxie.scrape_all:main (registered as `scrape-all` in pyproject.toml)
"""
import argparse
import logging
import sys

from moxie.scheduler.batch import run_batch
from moxie.scheduler.log_config import configure_logging


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Run a full batch scrape of all buildings."
    )
    mode_group = parser.add_mutually_exclusive_group()
    mode_group.add_argument(
        "--run-now",
        action="store_true",
        default=False,
        help="Run batch immediately, then exit (default if no mode specified)",
    )
    mode_group.add_argument(
        "--schedule",
        action="store_true",
        default=False,
        help="Enter scheduled mode: APScheduler fires at 2 AM Central daily (blocks)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=False,
        help="List buildings to scrape without actually scraping",
    )
    parser.add_argument(
        "--skip-sync",
        action="store_true",
        default=False,
        help="Skip the Google Sheets building list sync step",
    )
    args = parser.parse_args()

    # Configure logging: rotating file + console
    configure_logging()
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        stream=sys.stdout,
    )

    # Default to run-now if neither --run-now nor --schedule specified
    if not args.schedule:
        # Immediate run mode
        results = run_batch(
            skip_sheets_sync=args.skip_sync,
            dry_run=args.dry_run,
        )

        successes = sum(1 for r in results if r["status"] == "success")
        failures = sum(1 for r in results if r["status"] == "failed")
        total_units = sum(r["unit_count"] for r in results)

        print(f"\nBatch complete: {successes} ok, {failures} failed, {total_units} total units")

        if failures > 0:
            print(f"\nFailed buildings:")
            for r in results:
                if r["status"] == "failed":
                    print(f"  {r['building_name']}: {r.get('error', 'unknown error')}")
    else:
        # Scheduled mode: APScheduler blocks, fires at 2 AM Central daily
        from zoneinfo import ZoneInfo
        from apscheduler.schedulers.blocking import BlockingScheduler
        from apscheduler.triggers.cron import CronTrigger

        scheduler = BlockingScheduler(timezone=ZoneInfo("America/Chicago"))
        scheduler.add_job(
            run_batch,
            CronTrigger(hour=2, minute=0),
            id="daily_scrape",
            name="Daily full-building scrape",
            misfire_grace_time=3600,   # Run within 1h of missed trigger
            coalesce=True,              # Only run once if multiple firings missed
            max_instances=1,            # Never run two batch jobs concurrently
        )

        logger = logging.getLogger("moxie.scheduler")
        next_run = scheduler.get_job("daily_scrape").next_run_time
        logger.info(f"Scheduler started. Next run: {next_run}")
        print(f"Scheduler started. Next run at {next_run}. Press Ctrl+C to stop.")

        try:
            scheduler.start()  # Blocks until KeyboardInterrupt or SystemExit
        except (KeyboardInterrupt, SystemExit):
            logger.info("Scheduler shutting down...")
            scheduler.shutdown()
            print("Scheduler stopped.")


if __name__ == "__main__":
    main()
```

**Important notes:**
- Import `zoneinfo`, `apscheduler` only inside the `--schedule` branch to avoid import cost on immediate runs.
- `misfire_grace_time=3600` means if the machine was asleep/off at 2 AM but wakes within 1 hour, the job still fires.
- `coalesce=True` means multiple missed firings (e.g., machine off for 3 days) result in one single run, not 3.
- `max_instances=1` prevents overlapping batch runs.
- `America/Chicago` timezone for Central Time (the user's timezone).
  </action>
  <verify>
1. Run `uv run scrape-all --run-now --skip-sync --dry-run` — should complete immediately with building list
2. Run `uv run scrape-all --schedule` — should print "Scheduler started. Next run at 02:00 Central..." and block. Press Ctrl+C to stop — should print "Scheduler stopped." and exit cleanly.
3. Verify APScheduler installed: `uv run python -c "import apscheduler; print(apscheduler.__version__)"` — should print 3.x version
4. Run full existing test suite: `uv run pytest tests/ -x -q` — all must pass
  </verify>
  <done>
`uv run scrape-all --schedule` enters APScheduler scheduled mode that fires run_batch at 2 AM Central daily. The process blocks until Ctrl+C. `uv run scrape-all` (or `--run-now`) runs immediately then exits. APScheduler 3.x is installed with proper misfire_grace_time, coalesce, and max_instances configuration for overnight unattended operation.
  </done>
</task>

</tasks>

<verification>
1. `uv run scrape-all --schedule` starts and shows next run time at 2 AM Central — Ctrl+C exits cleanly
2. `uv run scrape-all --run-now --skip-sync` scrapes buildings and produces a "Scrape Status" tab in Google Sheet
3. `logs/scrape_batch.log` exists and contains batch run entries with timestamps
4. `uv run python -c "import apscheduler; print(apscheduler.__version__)"` prints 3.x
5. `uv run pytest tests/ -x -q` — all existing tests pass
6. The scrape_runs table has entries for the test run
</verification>

<success_criteria>
- APScheduler 2 AM cron fires run_batch in scheduled mode
- Google Sheet "Scrape Status" tab is created/updated after each batch run with summary + per-building rows
- Availability tab is refreshed after each batch run
- Local log file at logs/scrape_batch.log with 5MB rotation and 7 backups
- scrape_runs older than 30 days are pruned after each batch
- --run-now (default) and --schedule modes both work correctly
- --dry-run and --skip-sync flags work in both modes
</success_criteria>

<output>
After completion, create `.planning/phases/03-scheduler/03-02-SUMMARY.md`
</output>
