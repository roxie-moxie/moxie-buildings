---
phase: 03-scheduler
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/moxie/scrapers/registry.py
  - src/moxie/scrape.py
  - src/moxie/sync/push_availability.py
  - src/moxie/db/session.py
  - src/moxie/scheduler/__init__.py
  - src/moxie/scheduler/runner.py
  - src/moxie/scheduler/batch.py
  - src/moxie/scrape_all.py
  - pyproject.toml
autonomous: true
requirements:
  - INFRA-02

must_haves:
  truths:
    - "Running `uv run scrape-all --run-now` scrapes all non-dead buildings and writes results to the DB"
    - "Each building failure is isolated — one scraper crash does not stop other buildings from being scraped"
    - "Browser-based scrapers (rentcafe, groupfox, llm, entrata, mri, funnel, bozzuto) run one at a time per platform via semaphore"
    - "HTTP-based scrapers (sightmap, appfolio) run with concurrency of 2 per platform"
    - "On batch failure, units for the failed building are cleared (not retained as stale data)"
    - "SQLite WAL mode is enabled for safe concurrent thread access during batch runs"
  artifacts:
    - path: "src/moxie/scrapers/registry.py"
      provides: "Single source of truth for PLATFORM_SCRAPERS dict"
      contains: "PLATFORM_SCRAPERS"
    - path: "src/moxie/scheduler/runner.py"
      provides: "Per-building scrape wrapper with error isolation and clear-on-failure"
      contains: "scrape_one_building"
    - path: "src/moxie/scheduler/batch.py"
      provides: "Batch orchestrator: sheets_sync -> thread pool scrape -> results"
      contains: "run_batch"
    - path: "src/moxie/scrape_all.py"
      provides: "CLI entrypoint for scrape-all command"
      contains: "main"
  key_links:
    - from: "src/moxie/scheduler/batch.py"
      to: "src/moxie/scheduler/runner.py"
      via: "ThreadPoolExecutor calling scrape_one_building"
      pattern: "pool\\.submit\\(scrape_one_building"
    - from: "src/moxie/scheduler/runner.py"
      to: "src/moxie/scrapers/registry.py"
      via: "importlib.import_module using PLATFORM_SCRAPERS"
      pattern: "PLATFORM_SCRAPERS"
    - from: "src/moxie/scrape_all.py"
      to: "src/moxie/scheduler/batch.py"
      via: "CLI calls run_batch()"
      pattern: "run_batch"
---

<objective>
Build the batch scraping infrastructure: centralized scraper registry, per-building runner with error isolation and clear-on-failure semantics, thread pool batch orchestrator with per-platform semaphores, SQLite WAL mode for concurrent writes, and the `scrape-all` CLI entrypoint.

Purpose: This is the core execution engine for INFRA-02. Without this, there is no automated batch scraping capability. The batch runner reuses existing per-building scraper modules while adding concurrency control, failure isolation, and the user-mandated "clear units on failure" behavior.

Output: Working `uv run scrape-all --run-now` command that scrapes all buildings using per-platform concurrency limits.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/03-scheduler/03-CONTEXT.md
@.planning/phases/03-scheduler/03-RESEARCH.md
@src/moxie/scrape.py
@src/moxie/sync/push_availability.py
@src/moxie/scrapers/base.py
@src/moxie/db/session.py
@src/moxie/db/models.py
@src/moxie/sync/sheets.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Centralize scraper registry + enable SQLite WAL mode</name>
  <files>
    src/moxie/scrapers/registry.py
    src/moxie/scrape.py
    src/moxie/sync/push_availability.py
    src/moxie/db/session.py
  </files>
  <action>
**1. Create `src/moxie/scrapers/registry.py`:**
Extract the duplicated `PLATFORM_SCRAPERS` dict (currently in both `scrape.py` and `push_availability.py` with a "keep in sync" comment) into a single source of truth. The dict maps platform string to module path string:

```python
PLATFORM_SCRAPERS: dict[str, str] = {
    "rentcafe": "moxie.scrapers.tier2.securecafe",
    "ppm":      "moxie.scrapers.tier1.ppm",
    "funnel":   "moxie.scrapers.tier2.funnel",
    "appfolio": "moxie.scrapers.tier2.appfolio",
    "bozzuto":  "moxie.scrapers.tier2.bozzuto",
    "realpage": "moxie.scrapers.tier2.realpage",
    "groupfox": "moxie.scrapers.tier2.groupfox",
    "sightmap": "moxie.scrapers.tier2.sightmap",
    "entrata":  "moxie.scrapers.tier3.llm",
    "mri":      "moxie.scrapers.tier3.llm",
    "llm":      "moxie.scrapers.tier3.llm",
}
```

Also add a helper list of platforms that should be excluded from batch runs (because they have no working scraper or are dead):

```python
SKIP_PLATFORMS: set[str] = {"dead", "needs_classification"}
```

**2. Update `src/moxie/scrape.py`:**
Remove the inline `PLATFORM_SCRAPERS` dict. Replace with:
```python
from moxie.scrapers.registry import PLATFORM_SCRAPERS
```
Keep everything else unchanged.

**3. Update `src/moxie/sync/push_availability.py`:**
Remove the inline `PLATFORM_SCRAPERS` dict and its "keep in sync" comment. Replace with:
```python
from moxie.scrapers.registry import PLATFORM_SCRAPERS
```
Keep everything else unchanged.

**4. Enable SQLite WAL mode in `src/moxie/db/session.py`:**
Add an event listener on the engine that sets WAL mode and busy_timeout when connecting to SQLite. This is critical for the batch runner's concurrent thread writes:

```python
from sqlalchemy import event

@event.listens_for(engine, "connect")
def _configure_sqlite(dbapi_conn, connection_record):
    """Enable WAL mode for concurrent reads/writes during batch scraping."""
    if DATABASE_URL.startswith("sqlite"):
        cursor = dbapi_conn.cursor()
        cursor.execute("PRAGMA journal_mode=WAL")
        cursor.execute("PRAGMA busy_timeout=30000")  # Wait up to 30s for write locks
        cursor.close()
```

Place the listener registration right after the `engine = create_engine(...)` line. The `if sqlite` guard ensures it's a no-op for non-SQLite databases.
  </action>
  <verify>
Run existing tests to confirm nothing broke: `uv run pytest tests/ -x -q`. All existing tests must pass. Then verify the registry import works: `uv run python -c "from moxie.scrapers.registry import PLATFORM_SCRAPERS; print(len(PLATFORM_SCRAPERS), 'platforms registered')"`
  </verify>
  <done>
PLATFORM_SCRAPERS dict exists in exactly one place (registry.py). Both scrape.py and push_availability.py import from registry.py. SQLite WAL mode is configured on engine connect. All existing tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 2: Build per-building runner + batch orchestrator + CLI entrypoint</name>
  <files>
    src/moxie/scheduler/__init__.py
    src/moxie/scheduler/runner.py
    src/moxie/scheduler/batch.py
    src/moxie/scrape_all.py
    pyproject.toml
  </files>
  <action>
**1. Create `src/moxie/scheduler/__init__.py`:**
Empty file (package marker).

**2. Create `src/moxie/scheduler/runner.py`:**
The per-building scrape wrapper. This function runs in a thread pool thread — each thread gets its own DB session and event loop.

```python
"""Per-building scrape wrapper with error isolation and clear-on-failure."""
import importlib
import logging
import time
from datetime import datetime, timezone

from moxie.db.models import Building, Unit, ScrapeRun
from moxie.db.session import SessionLocal
from moxie.scrapers.registry import PLATFORM_SCRAPERS
from moxie.normalizer import normalize
from pydantic import ValidationError

logger = logging.getLogger("moxie.scheduler")

# Inter-scrape delay per platform type (seconds)
BROWSER_DELAY = 1.0  # Crawl4AI/Playwright platforms — be polite
HTTP_DELAY = 0.2      # HTTP-only scrapers — lighter footprint

_BROWSER_PLATFORMS = {"rentcafe", "groupfox", "llm", "entrata", "mri", "funnel", "bozzuto", "ppm"}

def scrape_one_building(building_id: int, building_name: str, building_url: str, platform: str) -> dict:
    """
    Scrape a single building and save results to DB. Called from a thread pool thread.

    Args:
        building_id: Building primary key
        building_name: For logging/reporting
        building_url: For logging/reporting
        platform: Platform key from PLATFORM_SCRAPERS

    Returns:
        dict with keys: building_id, building_name, platform, status ("success"|"failed"),
        unit_count (int), error (str|None), scraped_at (str ISO)
    """
    now = datetime.now(timezone.utc)
    result = {
        "building_id": building_id,
        "building_name": building_name,
        "platform": platform,
        "status": "failed",
        "unit_count": 0,
        "error": None,
        "scraped_at": now.strftime("%Y-%m-%d %H:%M UTC"),
    }

    db = SessionLocal()
    try:
        building = db.query(Building).get(building_id)
        if building is None:
            result["error"] = f"Building ID {building_id} not found in DB"
            return result

        # Import and call the scraper
        mod = importlib.import_module(PLATFORM_SCRAPERS[platform])
        raw_units: list[dict] = mod.scrape(building)

        # Save success: delete old units, insert new normalized units
        db.query(Unit).filter(Unit.building_id == building.id).delete()

        saved_count = 0
        if raw_units:
            for raw in raw_units:
                try:
                    unit_dict = normalize(raw, building.id)
                    db.add(Unit(**unit_dict))
                    saved_count += 1
                except (ValidationError, ValueError):
                    pass  # Skip unparseable units

        # Update building status
        if saved_count > 0:
            building.consecutive_zero_count = 0
            building.last_scrape_status = "success"
        else:
            building.consecutive_zero_count = (building.consecutive_zero_count or 0) + 1
            if building.consecutive_zero_count >= 5:
                building.last_scrape_status = "needs_attention"
            else:
                building.last_scrape_status = "success"

        building.last_scraped_at = now

        # Log scrape run
        db.add(ScrapeRun(
            building_id=building.id,
            run_at=now,
            status="success",
            unit_count=saved_count,
        ))
        db.commit()

        result["status"] = "success"
        result["unit_count"] = saved_count
        logger.info(f"OK  {building_name}: {saved_count} units ({platform})")

    except Exception as e:
        db.rollback()
        error_msg = f"[{type(e).__name__}] {str(e)[:500]}"
        result["error"] = error_msg

        # Clear-on-failure: delete units (user decision — stale data is NOT real data)
        try:
            building = db.query(Building).get(building_id)
            if building:
                db.query(Unit).filter(Unit.building_id == building.id).delete()
                building.last_scrape_status = "failed"
                building.last_scraped_at = now
                db.add(ScrapeRun(
                    building_id=building.id,
                    run_at=now,
                    status="failed",
                    unit_count=0,
                    error_message=error_msg[:1000],
                ))
                db.commit()
        except Exception:
            logger.error(f"Failed to record failure for {building_name}: {e}")

        logger.warning(f"FAIL {building_name}: {error_msg} ({platform})")
    finally:
        db.close()

    # Inter-scrape delay (politeness)
    delay = BROWSER_DELAY if platform in _BROWSER_PLATFORMS else HTTP_DELAY
    time.sleep(delay)

    return result
```

Key design notes:
- Accept primitive args (building_id, name, url, platform) not ORM objects — ORM objects are not safe to pass across threads (detached session).
- Each thread creates its own `SessionLocal()` and closes it when done.
- On failure, existing units are deleted (user decision: "stale data is NOT real data").
- Inter-scrape delay is configurable per platform type.

**3. Create `src/moxie/scheduler/batch.py`:**
The batch orchestrator. Full cycle: sheets_sync -> fan out scrapes -> collect results.

```python
"""Batch scrape orchestrator: sheets_sync -> parallel scrape -> summary."""
import logging
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from datetime import datetime, timezone

from moxie.db.models import Building
from moxie.db.session import SessionLocal
from moxie.scrapers.registry import PLATFORM_SCRAPERS, SKIP_PLATFORMS
from moxie.scheduler.runner import scrape_one_building
from moxie.sync.sheets import sheets_sync

logger = logging.getLogger("moxie.scheduler")

# Per-platform concurrency: browser-based = 1, HTTP-based = 2
PLATFORM_CONCURRENCY: dict[str, int] = {
    "rentcafe": 1,   # Crawl4AI / Playwright
    "groupfox": 1,
    "llm":      1,
    "entrata":  1,
    "mri":      1,
    "funnel":   1,
    "bozzuto":  1,
    "ppm":      1,   # Shared page — serialize to avoid duplicate fetches
    "sightmap": 2,   # HTTP only
    "appfolio": 2,
    "realpage": 1,
}

# Thread-safe semaphores — created once, shared across threads
_semaphores: dict[str, threading.Semaphore] = {
    p: threading.Semaphore(n) for p, n in PLATFORM_CONCURRENCY.items()
}
_default_sem = threading.Semaphore(1)

MAX_WORKERS = 8  # Thread pool size — most threads block on I/O or semaphore


def _scrape_with_semaphore(building_id: int, name: str, url: str, platform: str) -> dict:
    """Acquire platform semaphore, then call scrape_one_building."""
    sem = _semaphores.get(platform, _default_sem)
    with sem:
        return scrape_one_building(building_id, name, url, platform)


def run_batch(*, skip_sheets_sync: bool = False, dry_run: bool = False) -> list[dict]:
    """
    Execute a full batch scrape cycle.

    1. Pull building list from Google Sheets (unless skip_sheets_sync=True)
    2. Fan out scrapes across threads with per-platform semaphores
    3. Return list of per-building result dicts

    Args:
        skip_sheets_sync: Skip the Sheets pull step (useful for testing)
        dry_run: Log which buildings would be scraped, but don't actually scrape

    Returns:
        List of result dicts from scrape_one_building
    """
    start_time = datetime.now(timezone.utc)
    logger.info("=== Batch scrape starting ===")

    # Step 1: Sheets sync (pull building list)
    if not skip_sheets_sync:
        logger.info("Step 1: Syncing building list from Google Sheets...")
        db = SessionLocal()
        try:
            sync_result = sheets_sync(db)
            logger.info(
                f"Sheets sync: added={sync_result['added']}, "
                f"updated={sync_result['updated']}, "
                f"deleted={sync_result['deleted']}, "
                f"skipped={sync_result['skipped']}"
            )
        except Exception as e:
            logger.error(f"Sheets sync failed: {e} — continuing with existing building list")
        finally:
            db.close()
    else:
        logger.info("Step 1: Skipping sheets sync (--skip-sync)")

    # Step 2: Load all scrapeable buildings
    db = SessionLocal()
    try:
        buildings = db.query(Building).filter(
            Building.platform.notin_(SKIP_PLATFORMS),
            Building.platform.isnot(None),
        ).all()

        # Build list of (id, name, url, platform) tuples — detach from session
        building_specs = []
        for b in buildings:
            platform = b.platform
            if platform not in PLATFORM_SCRAPERS:
                logger.debug(f"Skipping {b.name}: platform '{platform}' has no scraper")
                continue
            building_specs.append((b.id, b.name, b.url, platform))
    finally:
        db.close()

    logger.info(f"Step 2: {len(building_specs)} buildings to scrape")

    if dry_run:
        logger.info("DRY RUN — listing buildings without scraping:")
        results = []
        for bid, name, url, platform in building_specs:
            logger.info(f"  [dry-run] {name} ({platform})")
            results.append({
                "building_id": bid,
                "building_name": name,
                "platform": platform,
                "status": "dry_run",
                "unit_count": 0,
                "error": None,
                "scraped_at": start_time.strftime("%Y-%m-%d %H:%M UTC"),
            })
        return results

    # Step 3: Fan out scrapes with thread pool
    logger.info(f"Step 3: Scraping with {MAX_WORKERS} workers...")
    results = []
    completed = 0
    total = len(building_specs)

    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as pool:
        futures = {
            pool.submit(_scrape_with_semaphore, bid, name, url, platform): (bid, name)
            for bid, name, url, platform in building_specs
        }
        for future in as_completed(futures):
            bid, name = futures[future]
            try:
                result = future.result()
            except Exception as e:
                # Should not reach here — scrape_one_building handles all exceptions
                result = {
                    "building_id": bid,
                    "building_name": name,
                    "platform": "unknown",
                    "status": "error",
                    "unit_count": 0,
                    "error": f"Unhandled: {e}",
                    "scraped_at": datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M UTC"),
                }
            results.append(result)
            completed += 1
            if completed % 50 == 0 or completed == total:
                logger.info(f"  Progress: {completed}/{total}")

    # Summary
    elapsed = (datetime.now(timezone.utc) - start_time).total_seconds()
    successes = sum(1 for r in results if r["status"] == "success")
    failures = sum(1 for r in results if r["status"] == "failed")
    total_units = sum(r["unit_count"] for r in results)
    logger.info(
        f"=== Batch complete: {successes} ok, {failures} failed, "
        f"{total_units} total units, {elapsed:.0f}s elapsed ==="
    )

    return results
```

**4. Create `src/moxie/scrape_all.py`:**
CLI entrypoint with `--run-now` for immediate execution (default behavior for the CLI). The scheduled mode (APScheduler) is added in Plan 02.

```python
"""
Batch scrape CLI entrypoint.

Usage:
    scrape-all                    # Run batch immediately, then exit
    scrape-all --run-now          # Same as above (explicit)
    scrape-all --dry-run          # List buildings without scraping
    scrape-all --skip-sync        # Skip Google Sheets sync step

Entrypoint: moxie.scrape_all:main (registered as `scrape-all` in pyproject.toml)
"""
import argparse
import logging
import sys

from moxie.scheduler.batch import run_batch


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Run a full batch scrape of all buildings."
    )
    parser.add_argument(
        "--run-now",
        action="store_true",
        default=True,
        help="Run batch immediately, then exit (default behavior)",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        default=False,
        help="List buildings to scrape without actually scraping",
    )
    parser.add_argument(
        "--skip-sync",
        action="store_true",
        default=False,
        help="Skip the Google Sheets building list sync step",
    )
    args = parser.parse_args()

    # Configure console logging
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s %(name)s: %(message)s",
        stream=sys.stdout,
    )

    results = run_batch(
        skip_sheets_sync=args.skip_sync,
        dry_run=args.dry_run,
    )

    # Print summary
    successes = sum(1 for r in results if r["status"] == "success")
    failures = sum(1 for r in results if r["status"] == "failed")
    total_units = sum(r["unit_count"] for r in results)

    print(f"\nBatch complete: {successes} ok, {failures} failed, {total_units} total units")

    if failures > 0:
        print(f"\nFailed buildings:")
        for r in results:
            if r["status"] == "failed":
                print(f"  {r['building_name']}: {r.get('error', 'unknown error')}")


if __name__ == "__main__":
    main()
```

**5. Register CLI in `pyproject.toml`:**
Add to `[project.scripts]`:
```toml
scrape-all = "moxie.scrape_all:main"
```
  </action>
  <verify>
1. Run `uv run scrape-all --dry-run --skip-sync` — should list all scrapeable buildings grouped by platform without actually scraping. Verify it exits cleanly with a count.
2. Run `uv run scrape-all --skip-sync` with a small subset (e.g., manually set all but 2-3 buildings to "dead" temporarily, or just run and observe the first few completions then Ctrl+C).
3. Run existing tests: `uv run pytest tests/ -x -q` — all must pass (registry import change should be transparent).
  </verify>
  <done>
`uv run scrape-all --run-now` (or just `uv run scrape-all`) executes the full batch: loads buildings from DB, fans out scrapes across threads with per-platform semaphores, saves results per building, and prints a summary. `--dry-run` lists buildings without scraping. `--skip-sync` bypasses Sheets pull. Failures are isolated per building and units are cleared on failure.
  </done>
</task>

</tasks>

<verification>
1. `uv run python -c "from moxie.scrapers.registry import PLATFORM_SCRAPERS; print(len(PLATFORM_SCRAPERS))"` prints 11
2. `uv run scrape-all --dry-run --skip-sync` lists all scrapeable buildings and exits 0
3. `uv run pytest tests/ -x -q` — all existing tests pass
4. `uv run python -c "from moxie.db.session import engine; conn = engine.raw_connection(); print(conn.execute('PRAGMA journal_mode').fetchone())"` prints `('wal',)` confirming WAL mode
</verification>

<success_criteria>
- PLATFORM_SCRAPERS exists in exactly one module (registry.py)
- scrape.py and push_availability.py import from registry.py
- SQLite WAL mode is active with busy_timeout=30000
- scrape-all CLI is registered in pyproject.toml and runs
- Batch runner uses ThreadPoolExecutor with per-platform semaphores
- Building failures are isolated (try/except per building)
- Units are cleared on failure (user decision: stale data = no data)
</success_criteria>

<output>
After completion, create `.planning/phases/03-scheduler/03-01-SUMMARY.md`
</output>
