---
phase: 02-scrapers
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/moxie/db/models.py
  - alembic/versions/add_consecutive_zero_count.py
  - src/moxie/scrapers/__init__.py
  - src/moxie/scrapers/base.py
  - src/moxie/scrapers/platform_detect.py
  - src/moxie/scrapers/tier1/__init__.py
  - src/moxie/scrapers/tier2/__init__.py
  - src/moxie/scrapers/tier3/__init__.py
  - pyproject.toml
autonomous: true
requirements: [INFRA-03]

must_haves:
  truths:
    - "Any scraper can call save_scrape_result(db, building, raw_units, scrape_succeeded=True) and units are written to DB via normalize()"
    - "On scrape failure (scrape_succeeded=False), existing unit records are retained and last_scrape_status is set to 'failed'"
    - "On zero-unit success, consecutive_zero_count increments; on non-zero success it resets to 0"
    - "detect_platform(url) returns the correct platform string for all 8 known domains and None for unrecognized URLs"
    - "alembic upgrade head succeeds with consecutive_zero_count column present on buildings table"
    - "httpx, beautifulsoup4, crawl4ai, lxml, anthropic are importable in the uv environment"
  artifacts:
    - path: "src/moxie/db/models.py"
      provides: "Building model with consecutive_zero_count field"
      contains: "consecutive_zero_count"
    - path: "src/moxie/scrapers/base.py"
      provides: "ScraperProtocol typing.Protocol and save_scrape_result() function"
      exports: ["ScraperProtocol", "save_scrape_result", "CONSECUTIVE_ZERO_THRESHOLD"]
    - path: "src/moxie/scrapers/platform_detect.py"
      provides: "detect_platform(url) → platform string or None"
      exports: ["detect_platform", "PLATFORM_PATTERNS"]
    - path: "alembic/versions/add_consecutive_zero_count.py"
      provides: "Migration adding consecutive_zero_count column to buildings"
      contains: "consecutive_zero_count"
  key_links:
    - from: "src/moxie/scrapers/base.py"
      to: "src/moxie/normalizer.py"
      via: "normalize() call inside save_scrape_result()"
      pattern: "from moxie\\.normalizer import normalize"
    - from: "src/moxie/scrapers/base.py"
      to: "src/moxie/db/models.py"
      via: "Building.consecutive_zero_count attribute access"
      pattern: "building\\.consecutive_zero_count"
    - from: "alembic/versions/add_consecutive_zero_count.py"
      to: "src/moxie/db/models.py"
      via: "Both add consecutive_zero_count — must be consistent"
      pattern: "consecutive_zero_count"
---

<objective>
Install scraper dependencies, add the consecutive_zero_count schema migration, and create the scraper infrastructure modules (base.py with save_scrape_result and platform_detect.py) that every scraper in the phase depends on.

Purpose: All Phase 2 scraper modules depend on save_scrape_result() for DB writes, platform_detect.detect_platform() for URL classification, and the consecutive_zero_count Building column for zero-unit tracking. These must exist before any scraper plan runs.

Output: pyproject.toml updated with new deps, Alembic migration for consecutive_zero_count, models.py updated, scrapers/ package tree initialized, base.py with ScraperProtocol + save_scrape_result(), platform_detect.py with detect_platform().
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-CONTEXT.md
@.planning/phases/02-scrapers/02-RESEARCH.md
@src/moxie/db/models.py
@src/moxie/normalizer.py
@src/moxie/db/session.py
@alembic/versions/50fb02b298b3_initial_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Install scraper deps, add consecutive_zero_count migration, and update Building model</name>
  <files>
    pyproject.toml
    alembic/versions/add_consecutive_zero_count.py
    src/moxie/db/models.py
  </files>
  <action>
    1. Add dependencies to pyproject.toml:
       - In [project] dependencies: add "httpx>=0.28.1", "beautifulsoup4>=4.14.0", "crawl4ai>=0.8.0", "lxml>=5.0", "anthropic>=0.40.0"
       - In [dependency-groups] dev: add "pytest-httpx>=0.35.0"

    2. Run: uv sync

    3. Run: crawl4ai-setup
       (This installs Playwright browsers. If it fails with a path/permission error, note the error but continue — it can be re-run during execution.)

    4. Create Alembic migration for consecutive_zero_count:
       Run: uv run alembic revision --autogenerate -m "add_consecutive_zero_count"
       Then edit the generated file to ensure upgrade() uses batch_alter_table (required for SQLite):
       ```python
       def upgrade() -> None:
           with op.batch_alter_table("buildings") as batch_op:
               batch_op.add_column(
                   sa.Column("consecutive_zero_count", sa.Integer(), server_default="0", nullable=False)
               )

       def downgrade() -> None:
           with op.batch_alter_table("buildings") as batch_op:
               batch_op.drop_column("consecutive_zero_count")
       ```
       If autogenerate produces the correct batch_alter_table pattern, leave it. If autogenerate produces op.add_column directly (not in a batch context), replace with the batch version above.

    5. Add consecutive_zero_count to the Building model in src/moxie/db/models.py, after last_scraped_at:
       ```python
       consecutive_zero_count: Mapped[int] = mapped_column(Integer, default=0, server_default="0", nullable=False)
       ```

    6. Run the migration: uv run alembic upgrade head
       Verify success: uv run alembic current (should show the new revision as head)
  </action>
  <verify>
    - uv sync exits 0 with no dependency errors
    - uv run alembic upgrade head exits 0
    - uv run alembic current shows the new migration as "(head)"
    - sqlite3 moxie.db "PRAGMA table_info(buildings);" | grep consecutive_zero_count shows the column
    - uv run python -c "import httpx, bs4, crawl4ai, lxml, anthropic; print('ok')" exits 0
  </verify>
  <done>New deps importable, consecutive_zero_count column in buildings table, Building model has the field, migration history is clean with no pending heads.</done>
</task>

<task type="auto">
  <name>Task 2: Create scrapers package with base.py (ScraperProtocol + save_scrape_result) and platform_detect.py</name>
  <files>
    src/moxie/scrapers/__init__.py
    src/moxie/scrapers/base.py
    src/moxie/scrapers/platform_detect.py
    src/moxie/scrapers/tier1/__init__.py
    src/moxie/scrapers/tier2/__init__.py
    src/moxie/scrapers/tier3/__init__.py
  </files>
  <action>
    1. Create src/moxie/scrapers/__init__.py — empty (package marker only).

    2. Create src/moxie/scrapers/tier1/__init__.py, tier2/__init__.py, tier3/__init__.py — all empty.

    3. Create src/moxie/scrapers/base.py:

    ```python
    """
    Scraper base infrastructure.

    ScraperProtocol: typing.Protocol that all scraper modules satisfy structurally.
    save_scrape_result(): centralized DB write function called by all scrapers.
    """
    from datetime import datetime, timezone
    from typing import Protocol
    from sqlalchemy.orm import Session
    from moxie.db.models import Building, Unit, ScrapeRun
    from moxie.normalizer import normalize

    CONSECUTIVE_ZERO_THRESHOLD = 5


    class ScraperProtocol(Protocol):
        def scrape(self, building: Building) -> list[dict]:
            """Return list of raw unit dicts (pre-normalization). Empty list = no units."""
            ...


    def save_scrape_result(
        db: Session,
        building: Building,
        raw_units: list[dict],
        *,
        scrape_succeeded: bool,
        error_message: str | None = None,
    ) -> None:
        """
        Write scrape results to the database. Called by all scrapers after scrape().

        On success (scrape_succeeded=True):
          - Deletes all existing units for this building (delete then re-insert)
          - If raw_units non-empty: inserts normalized units, resets consecutive_zero_count to 0
          - If raw_units empty: increments consecutive_zero_count; sets last_scrape_status
            to 'needs_attention' after CONSECUTIVE_ZERO_THRESHOLD consecutive zeros
          - Sets last_scrape_status='success' (or 'needs_attention' at threshold)
          - Sets last_scraped_at=now

        On failure (scrape_succeeded=False):
          - Retains existing units (no delete)
          - Sets last_scrape_status='failed', last_scraped_at=now
          - Does NOT increment consecutive_zero_count (errors != zero-unit success)

        Always logs a ScrapeRun record.
        """
        now = datetime.now(timezone.utc)

        if scrape_succeeded:
            db.query(Unit).filter(Unit.building_id == building.id).delete()

            if raw_units:
                for raw in raw_units:
                    unit_dict = normalize(raw, building.id)
                    db.add(Unit(**unit_dict))
                building.consecutive_zero_count = 0
                building.last_scrape_status = "success"
            else:
                building.consecutive_zero_count = (building.consecutive_zero_count or 0) + 1
                if building.consecutive_zero_count >= CONSECUTIVE_ZERO_THRESHOLD:
                    building.last_scrape_status = "needs_attention"
                else:
                    building.last_scrape_status = "success"

            building.last_scraped_at = now
        else:
            building.last_scrape_status = "failed"
            building.last_scraped_at = now

        db.add(ScrapeRun(
            building_id=building.id,
            run_at=now,
            status="success" if scrape_succeeded else "failed",
            unit_count=len(raw_units) if scrape_succeeded else 0,
            error_message=error_message,
        ))
        db.commit()
    ```

    4. Create src/moxie/scrapers/platform_detect.py:

    ```python
    """
    Platform detection — URL pattern matching to classify buildings by scraper platform.

    detect_platform(url) returns a platform string or None.
    None means the building should be assigned platform='llm' (catch-all).

    Platform strings:
      rentcafe  — RentCafe/Yardi (rentcafe.com)
      ppm       — PPM Apartments (ppmapartments.com)
      funnel    — Funnel/Nestio (nestiolistings.com, funnelleasing.com)
      realpage  — RealPage/G5 (realpage.com, g5searchmarketing.com)
      bozzuto   — Bozzuto (bozzuto.com)
      groupfox  — Groupfox (groupfox.com)
      appfolio  — AppFolio (appfolio.com)
      llm       — everything else (assigned by caller when detect_platform returns None)
    """
    from urllib.parse import urlparse

    # Ordered list: first match wins. More specific patterns before less specific.
    PLATFORM_PATTERNS: list[tuple[str, str]] = [
        ("rentcafe", "rentcafe.com"),
        ("ppm", "ppmapartments.com"),
        ("funnel", "nestiolistings.com"),
        ("funnel", "funnelleasing.com"),
        ("realpage", "realpage.com"),
        ("realpage", "g5searchmarketing.com"),
        ("bozzuto", "bozzuto.com"),
        ("groupfox", "groupfox.com"),
        ("appfolio", "appfolio.com"),
    ]

    KNOWN_PLATFORMS: frozenset[str] = frozenset({
        "rentcafe", "ppm", "funnel", "realpage", "bozzuto", "groupfox", "appfolio", "llm"
    })


    def detect_platform(url: str) -> str | None:
        """
        Return the platform string for a given building URL, or None if unrecognized.

        None should be treated as 'llm' by the caller (sheets_sync or manual assignment).
        Only runs URL pattern matching — no HTTP requests.

        Args:
            url: Full URL string (e.g. "https://somebuilding.rentcafe.com/...")

        Returns:
            Platform string (e.g. "rentcafe") or None if no pattern matched.
        """
        if not url:
            return None
        try:
            parsed = urlparse(url.lower())
            hostname = parsed.netloc or parsed.path
        except Exception:
            return None
        for platform, pattern in PLATFORM_PATTERNS:
            if pattern in hostname:
                return platform
        return None
    ```
  </action>
  <verify>
    - uv run python -c "from moxie.scrapers.base import ScraperProtocol, save_scrape_result, CONSECUTIVE_ZERO_THRESHOLD; print('ok')" exits 0
    - uv run python -c "from moxie.scrapers.platform_detect import detect_platform; assert detect_platform('https://foo.rentcafe.com/') == 'rentcafe'; assert detect_platform('https://ppmapartments.com/availability/') == 'ppm'; assert detect_platform('https://example.com') is None; print('ok')" exits 0
    - uv run pytest tests/ -v exits 0 (existing Phase 1 tests still pass)
  </verify>
  <done>scrapers/ package importable, save_scrape_result() usable by any scraper, detect_platform() classifies all 8 known platforms correctly and returns None for unknowns.</done>
</task>

</tasks>

<verification>
Run after both tasks complete:
- uv run alembic upgrade head (idempotent, should show "Already at head")
- sqlite3 moxie.db "PRAGMA table_info(buildings);" (consecutive_zero_count must appear)
- uv run python -c "from moxie.scrapers.base import save_scrape_result; from moxie.scrapers.platform_detect import detect_platform; print('imports ok')"
- uv run pytest tests/ -v (all Phase 1 tests still green)
</verification>

<success_criteria>
- consecutive_zero_count column exists in buildings table with default 0
- Building model has consecutive_zero_count Mapped field
- save_scrape_result() correctly separates success/failure/zero-unit paths
- detect_platform() handles all 8 platform domains and returns None for unknowns
- All new libs importable (httpx, bs4, crawl4ai, lxml, anthropic)
- Phase 1 tests still pass (no regressions)
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-01-SUMMARY.md`
</output>
