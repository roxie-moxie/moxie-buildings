---
phase: 02-scrapers
plan: 04
type: execute
wave: 3
depends_on: [02-01]
files_modified:
  - src/moxie/scrapers/tier1/rentcafe.py
  - src/moxie/scrapers/tier1/ppm.py
  - tests/test_scraper_rentcafe.py
  - tests/test_scraper_ppm.py
autonomous: true
requirements: [SCRAP-01, SCRAP-03]

must_haves:
  truths:
    - "rentcafe.scrape(building) raises NotImplementedError with a clear message when rentcafe_property_id or rentcafe_api_token is missing (stub behavior)"
    - "rentcafe.scrape(building) raises ValueError with message containing 'RentCafe API error' when API returns an Error:1020 response (even in stub — tested against mock)"
    - "ppm.scrape(building) calls the PPM availability URL exactly once per Building object, filters results by building name (partial match), and returns only that building's units"
    - "ppm.scrape() handles the JS-rendered page using Crawl4AI (AsyncWebCrawler), not plain httpx"
    - "PPM building name matching uses case-insensitive partial/contains match, not exact equality"
  artifacts:
    - path: "src/moxie/scrapers/tier1/rentcafe.py"
      provides: "RentCafe/Yardi scraper with stubbed API call and Error:1020 guard"
      exports: ["scrape"]
    - path: "src/moxie/scrapers/tier1/ppm.py"
      provides: "PPM single-page scraper using Crawl4AI + BeautifulSoup"
      exports: ["scrape"]
    - path: "tests/test_scraper_rentcafe.py"
      provides: "Tests for RentCafe scraper stub behavior and Error:1020 detection"
      min_lines: 30
    - path: "tests/test_scraper_ppm.py"
      provides: "Tests for PPM name matching and unit filtering logic"
      min_lines: 30
  key_links:
    - from: "src/moxie/scrapers/tier1/rentcafe.py"
      to: "src/moxie/db/models.py"
      via: "Building.rentcafe_property_id and Building.rentcafe_api_token access"
      pattern: "building\\.rentcafe_"
    - from: "src/moxie/scrapers/tier1/ppm.py"
      to: "crawl4ai"
      via: "AsyncWebCrawler for JS rendering"
      pattern: "AsyncWebCrawler"

user_setup:
  - service: rentcafe
    why: "RentCafe API credentials are per-property and must be discovered via a spike before the stub can be replaced with a real implementation"
    env_vars:
      - name: RENTCAFE_SPIKE_PENDING
        source: "Not an env var — a reminder. The RentCafe scraper is STUBBED. To activate: (1) fetch 2-3 known RentCafe building URLs from the DB, (2) inspect HTML/JS for companyCode, propertyCode, apiToken values, (3) verify requestType=apartmentavailability returns unit-level fields, (4) replace stub in rentcafe.py with real httpx call."
---

<objective>
Build Tier 1 scrapers: RentCafe/Yardi (stubbed API with Error:1020 guard) and PPM (Crawl4AI + BeautifulSoup single-page scraper). Both produce list[dict] for save_scrape_result(). The RentCafe stub is ready to swap in real credentials once the API spike is done.

Purpose: RentCafe covers ~220 buildings (55% of the total). The stub implementation gives Phase 3 a callable module to dispatch against. PPM's single-page pattern is confirmed — one HTTP call covers all 18 buildings.

Output: tier1/rentcafe.py (stubbed), tier1/ppm.py (functional), tests for both.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-01-SUMMARY.md
@src/moxie/db/models.py
@src/moxie/scrapers/base.py
@src/moxie/normalizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RentCafe/Yardi scraper (stubbed) with Error:1020 guard</name>
  <files>
    src/moxie/scrapers/tier1/rentcafe.py
    tests/test_scraper_rentcafe.py
  </files>
  <action>
    Create src/moxie/scrapers/tier1/rentcafe.py:

    ```python
    """
    RentCafe/Yardi scraper — Tier 1 REST API.

    STATUS: STUBBED — API call is not implemented. The scraper structure is complete
    and ready for real credentials. To activate:
    1. Run the RentCafe credential spike (see RESEARCH.md Open Question 1)
    2. Confirm requestType=apartmentavailability field names
    3. Replace _fetch_units() stub with the real httpx call
    4. Populate rentcafe_property_id and rentcafe_api_token on buildings via DB update

    Platform: 'rentcafe'
    Coverage: ~220 buildings (55% of total)
    API: https://api.rentcafe.com/rentcafeapi.aspx
    """
    import httpx
    from moxie.db.models import Building

    RENTCAFE_API_BASE = "https://api.rentcafe.com/rentcafeapi.aspx"

    # Known error response from RentCafe API for invalid/missing credentials
    _RENTCAFE_ERROR_KEY = "Error"


    class RentCafeCredentialError(ValueError):
        """Raised when a building is missing RentCafe API credentials."""


    class RentCafeAPIError(RuntimeError):
        """Raised when the RentCafe API returns an error response (e.g. Error:1020)."""


    def _check_for_api_error(data: list) -> None:
        """Raise RentCafeAPIError if response contains an error object."""
        if isinstance(data, list) and data and _RENTCAFE_ERROR_KEY in data[0]:
            raise RentCafeAPIError(f"RentCafe API error: {data[0][_RENTCAFE_ERROR_KEY]}")


    def _fetch_units(property_code: str, api_token: str) -> list[dict]:
        """
        STUBBED: Call the RentCafe apartmentavailability endpoint.

        Replace this stub once credentials and field names are confirmed via spike.
        The real implementation uses requestType=apartmentavailability with
        companyCode, propertyCode, apiToken, showallunit=1.

        Spike task: fetch 2-3 known RentCafe building URLs from DB, extract credentials
        from HTML/JS, hit the real endpoint, document exact field names.
        """
        raise NotImplementedError(
            f"RentCafe API stub — credentials not yet confirmed. "
            f"property_code={property_code!r}. "
            "Run the RentCafe credential spike before replacing this stub."
        )


    def _map_unit(raw: dict) -> dict:
        """
        Map RentCafe API response fields to UnitInput dict shape.

        Field mapping (from RESEARCH.md + spike to confirm apartmentavailability fields):
          UnitNumber / ApartmentNumber → unit_number
          Beds / Bedrooms             → bed_type
          Rent / MinimumRent          → rent
          AvailableDate               → availability_date
          FloorplanName               → floor_plan_name
          Baths / Bathrooms           → baths
          SQFT                        → sqft

        NOTE: Exact field names for apartmentavailability endpoint are TBD (spike needed).
        This mapper uses the most likely field names based on RESEARCH.md findings.
        """
        return {
            "unit_number": raw.get("UnitNumber") or raw.get("ApartmentNumber", ""),
            "bed_type": str(raw.get("Beds") or raw.get("Bedrooms", "")),
            "rent": raw.get("Rent") or raw.get("MinimumRent", "0"),
            "availability_date": raw.get("AvailableDate") or raw.get("AvailabilityDate", "Available Now"),
            "floor_plan_name": raw.get("FloorplanName"),
            "baths": str(raw.get("Baths") or raw.get("Bathrooms", "")) or None,
            "sqft": raw.get("SQFT"),
        }


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability for a RentCafe/Yardi building.

        Requires building.rentcafe_property_id and building.rentcafe_api_token.
        Returns list of raw unit dicts suitable for normalize() / save_scrape_result().

        Raises:
            RentCafeCredentialError: if credentials are missing on the building record.
            RentCafeAPIError: if the API returns an error response (e.g. Error:1020).
            NotImplementedError: until the stub is replaced with a real httpx call.
        """
        if not building.rentcafe_property_id or not building.rentcafe_api_token:
            raise RentCafeCredentialError(
                f"Building {building.id} ({building.name!r}) is missing RentCafe credentials. "
                "Set rentcafe_property_id and rentcafe_api_token on the building record."
            )

        raw_response = _fetch_units(
            property_code=building.rentcafe_property_id,
            api_token=building.rentcafe_api_token,
        )

        _check_for_api_error(raw_response)
        return [_map_unit(item) for item in raw_response]
    ```

    Create tests/test_scraper_rentcafe.py:
    - test_scrape_raises_credential_error_when_property_id_missing: building with no rentcafe_property_id → RentCafeCredentialError
    - test_scrape_raises_credential_error_when_api_token_missing: no rentcafe_api_token → RentCafeCredentialError
    - test_check_for_api_error_raises_on_error_1020: _check_for_api_error([{"Error": "1020"}]) → RentCafeAPIError
    - test_check_for_api_error_passes_on_valid_data: _check_for_api_error([{"UnitNumber": "101"}]) → no exception
    - test_map_unit_basic: _map_unit with known fields → correct dict shape for UnitInput
    - test_scrape_raises_not_implemented_when_stub_active: building with valid credentials → NotImplementedError (confirms stub is in place)

    Use in-memory Building objects (not DB-backed) for the credential tests — just instantiate Building(rentcafe_property_id=None, ...) and pass to scrape().
  </action>
  <verify>
    uv run pytest tests/test_scraper_rentcafe.py -v (all 6 tests pass)
  </verify>
  <done>RentCafe scraper stub is ready. Error:1020 guard confirmed by test. Missing credential check prevents silent failure. Stub raises NotImplementedError with a clear upgrade path message.</done>
</task>

<task type="auto">
  <name>Task 2: PPM single-page scraper (Crawl4AI + BeautifulSoup)</name>
  <files>
    src/moxie/scrapers/tier1/ppm.py
    tests/test_scraper_ppm.py
  </files>
  <action>
    Create src/moxie/scrapers/tier1/ppm.py:

    ```python
    """
    PPM Apartments scraper — Tier 1 single-page availability.

    PPM publishes all units for all buildings on one page:
    https://ppmapartments.com/availability/

    The page is JavaScript-rendered — unit rows are injected by JS after load.
    Crawl4AI (AsyncWebCrawler) renders the page, then BeautifulSoup parses the HTML.

    Design: Call the page ONCE per scraper run, cache in memory, filter per building.
    Do NOT call the page once per building (18 buildings × 1 call = wasteful).

    Platform: 'ppm'
    Coverage: ~18 buildings
    """
    import asyncio
    from functools import lru_cache
    from bs4 import BeautifulSoup
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
    from moxie.db.models import Building

    PPM_URL = "https://ppmapartments.com/availability/"

    # Table column indices (0-based) from the confirmed PPM table structure:
    # Neighborhood | Building | Unit | Availability | Unit Type | Floorplan | Features | Price
    _COL_BUILDING = 1
    _COL_UNIT = 2
    _COL_AVAILABILITY = 3
    _COL_UNIT_TYPE = 4
    _COL_FLOORPLAN = 5
    _COL_PRICE = 7


    async def _fetch_ppm_html() -> str:
        """Fetch and JS-render the PPM availability page. Returns full rendered HTML."""
        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(PPM_URL, config=config)
        return result.html or ""


    def _parse_ppm_html(html: str) -> list[dict]:
        """
        Parse the PPM availability table from rendered HTML.
        Returns a list of raw unit dicts (with 'building_name' field for filtering).
        """
        soup = BeautifulSoup(html, "html.parser")
        units = []
        # Find all table rows; skip header rows (th cells only)
        for row in soup.select("table tr"):
            cells = row.find_all("td")
            if len(cells) < 8:
                continue  # header row or empty row
            unit_type = cells[_COL_UNIT_TYPE].get_text(strip=True)
            if not unit_type:
                continue  # skip rows without unit type data
            units.append({
                "building_name": cells[_COL_BUILDING].get_text(strip=True),
                "unit_number": cells[_COL_UNIT].get_text(strip=True),
                "availability_date": cells[_COL_AVAILABILITY].get_text(strip=True) or "Available Now",
                "bed_type": unit_type,
                "floor_plan_name": cells[_COL_FLOORPLAN].get_text(strip=True) or None,
                "rent": cells[_COL_PRICE].get_text(strip=True),
            })
        return units


    def _matches_building(unit_building_name: str, building_name: str) -> bool:
        """
        Case-insensitive partial match: does the unit's building name contain
        (or is contained by) the DB building name?

        Handles cases where PPM uses "Streeterville Tower" but DB has "PPM - Streeterville Tower"
        or vice versa.
        """
        unit_lower = unit_building_name.lower().strip()
        db_lower = building_name.lower().strip()
        return unit_lower in db_lower or db_lower in unit_lower


    def _fetch_all_ppm_units() -> list[dict]:
        """Fetch and parse all PPM units. Run once, filter per building."""
        html = asyncio.run(_fetch_ppm_html())
        return _parse_ppm_html(html)


    def scrape(building: Building) -> list[dict]:
        """
        Return units for this PPM building from the shared availability page.

        IMPORTANT: This function calls the PPM availability page every time it is invoked.
        Phase 3 scheduler should call this once per full PPM batch and pass the cached
        result if needed. For now, each individual call fetches the full page.

        Returns list of raw unit dicts (without 'building_name' field) for normalize().
        """
        all_units = _fetch_all_ppm_units()
        matched = [
            {k: v for k, v in unit.items() if k != "building_name"}
            for unit in all_units
            if _matches_building(unit["building_name"], building.name)
        ]
        return matched
    ```

    Create tests/test_scraper_ppm.py — test the pure parsing and matching logic (not Crawl4AI):
    - test_parse_ppm_html_empty: empty HTML → []
    - test_parse_ppm_html_skips_header_rows: table with th-only rows → skipped
    - test_parse_ppm_html_extracts_units: valid rendered HTML with 2 rows → 2 dicts
    - test_matches_building_exact: "Tower" in "Tower" → True
    - test_matches_building_partial_db_has_prefix: "PPM - Streeterville Tower" contains "Streeterville Tower" → True
    - test_matches_building_partial_reverse: unit name is shorter than DB name → True
    - test_matches_building_no_match: completely different names → False
    - test_matches_building_case_insensitive: "TOWER" vs "tower" → True
    - test_scrape_filters_to_building: mock _fetch_all_ppm_units to return 3 units from different buildings → only matching ones returned
    - test_scrape_strips_building_name_from_output: returned dicts do NOT contain 'building_name' key

    Use monkeypatch to replace _fetch_all_ppm_units in tests — no real HTTP calls in test suite.
  </action>
  <verify>
    uv run pytest tests/test_scraper_ppm.py -v (all 10 tests pass)
    uv run pytest tests/ -v (full suite passes)
  </verify>
  <done>PPM scraper functional. Crawl4AI used for JS rendering. Partial case-insensitive name matching handles real-world name divergence. Tests cover all parsing and filtering logic without real HTTP calls.</done>
</task>

</tasks>

<verification>
- uv run pytest tests/test_scraper_rentcafe.py tests/test_scraper_ppm.py -v
- uv run pytest tests/ -v (full suite passes)
- uv run python -c "from moxie.scrapers.tier1.rentcafe import scrape; from moxie.scrapers.tier1.ppm import scrape; print('imports ok')"
</verification>

<success_criteria>
- RentCafe stub raises NotImplementedError (clear upgrade path) and RentCafeCredentialError (missing credentials)
- Error:1020 guard in _check_for_api_error() prevents silent zero-unit false positives
- PPM scraper uses Crawl4AI for JS rendering and BeautifulSoup for parsing
- PPM name matching uses partial case-insensitive contains logic
- All tests pass without real network calls
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-04-SUMMARY.md`
</output>
