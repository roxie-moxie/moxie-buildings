---
phase: 02-scrapers
plan: 08
type: execute
wave: 3
depends_on: [02-01]
files_modified:
  - src/moxie/scrapers/tier3/llm.py
  - tests/test_scraper_llm.py
autonomous: true
requirements: [SCRAP-02, SCRAP-09]

must_haves:
  truths:
    - "llm.scrape(building) uses Crawl4AI LLMExtractionStrategy with Claude Haiku to extract unit data from arbitrary HTML pages"
    - "The scraper works for both custom sites (~50-70 buildings) AND Entrata buildings (~30-40), since Entrata is routed to platform='llm'"
    - "Extraction instruction is specific enough to return unit_number, bed_type, rent, availability_date, floor_plan_name, baths, sqft"
    - "Empty extraction result (no units found by LLM) returns empty list — does NOT raise an error"
    - "Malformed JSON from LLM extraction is caught and returns empty list (does not crash)"
    - "ANTHROPIC_API_KEY environment variable is read via os.environ — not hardcoded"
  artifacts:
    - path: "src/moxie/scrapers/tier3/llm.py"
      provides: "Crawl4AI + Claude Haiku LLM extraction scraper for long-tail and Entrata buildings"
      exports: ["scrape"]
    - path: "tests/test_scraper_llm.py"
      provides: "Tests for LLM scraper JSON parsing, empty result, and malformed output handling"
      min_lines: 30
  key_links:
    - from: "src/moxie/scrapers/tier3/llm.py"
      to: "crawl4ai"
      via: "LLMExtractionStrategy + AsyncWebCrawler"
      pattern: "LLMExtractionStrategy"
    - from: "src/moxie/scrapers/tier3/llm.py"
      to: "ANTHROPIC_API_KEY"
      via: "os.environ lookup"
      pattern: "ANTHROPIC_API_KEY"

user_setup:
  - service: anthropic
    why: "LLM scraper requires Claude Haiku API access via ANTHROPIC_API_KEY"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console (console.anthropic.com) → API Keys → Create new key"
---

<objective>
Build the Tier 3 LLM fallback scraper using Crawl4AI + Claude Haiku. This scraper handles all buildings with platform='llm': ~50-70 custom/long-tail sites plus all Entrata buildings (~30-40). The scraper uses Crawl4AI's LLMExtractionStrategy with a Pydantic schema to extract structured unit data from arbitrary HTML.

Purpose: The LLM tier is the safety net for all buildings that don't fit a known platform pattern. Rather than building a custom scraper for every unique management site, one LLM module covers them all. Entrata buildings are explicitly routed here per the locked CONTEXT.md decision (no Entrata API scraper).

Output: tier3/llm.py with Crawl4AI LLMExtractionStrategy, tests for output parsing/error handling, ANTHROPIC_API_KEY documented in user_setup.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-01-SUMMARY.md
@src/moxie/db/models.py
@src/moxie/scrapers/base.py
@src/moxie/normalizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: LLM fallback scraper (Crawl4AI + Claude Haiku)</name>
  <files>
    src/moxie/scrapers/tier3/llm.py
    tests/test_scraper_llm.py
  </files>
  <action>
    Create src/moxie/scrapers/tier3/llm.py:

    ```python
    """
    LLM fallback scraper — Tier 3 (Crawl4AI + Claude Haiku).

    Covers:
    - ~50-70 custom/long-tail sites with no recognized platform pattern
    - All Entrata buildings (~30-40) — routed here per CONTEXT.md decision
      (no Entrata API scraper; revisit as Phase 2.x if LLM struggles)

    How it works:
    1. Crawl4AI fetches and renders the building's URL (handles JS)
    2. Crawl4AI converts HTML to markdown (5-10x token reduction vs raw HTML)
    3. LLMExtractionStrategy sends markdown to Claude Haiku with a Pydantic schema
    4. Claude Haiku returns a JSON list of UnitRecord objects
    5. Scraper returns the list for normalize() / save_scrape_result()

    Cost estimate (Claude Haiku 3, as of 2026-02-18):
    - ~5,000-20,000 tokens per page → ~$0.15-$0.30/day for 60 buildings
    - ~$4.50-$9/month — well under the $120/month target

    Provider: "anthropic/claude-3-haiku-20240307" (via LiteLLM in Crawl4AI)
    Requires: ANTHROPIC_API_KEY in environment

    Platform: 'llm'
    Coverage: ~80-110 buildings (custom sites + Entrata)
    """
    import asyncio
    import json
    import os
    from typing import Optional
    from pydantic import BaseModel
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode, LLMConfig
    from crawl4ai.extraction_strategy import LLMExtractionStrategy
    from moxie.db.models import Building

    # Claude Haiku model via LiteLLM provider string
    _HAIKU_PROVIDER = "anthropic/claude-3-haiku-20240307"

    # Structured extraction schema — matches UnitInput fields in normalizer.py
    class _UnitRecord(BaseModel):
        unit_number: str
        bed_type: str
        rent: str  # raw string; normalizer handles "$1,500/mo", "1500", etc.
        availability_date: str  # raw string; normalizer parses all formats
        floor_plan_name: Optional[str] = None
        baths: Optional[str] = None
        sqft: Optional[str] = None

    _EXTRACTION_INSTRUCTION = (
        "Extract all apartment units currently available for rent from this page. "
        "For each available unit, extract: "
        "unit_number (the unit identifier, e.g. '101', 'A3', 'Studio-2'), "
        "bed_type (e.g. 'Studio', '1 Bedroom', '2BR', 'Convertible'), "
        "rent (monthly price as a string, e.g. '$1,800/mo', '2500'), "
        "availability_date (move-in date as a string, e.g. 'Available Now', 'March 1, 2026', '2026-04-01'), "
        "floor_plan_name (name of the floor plan if shown, otherwise null), "
        "baths (number of bathrooms as a string if shown, otherwise null), "
        "sqft (square footage as a string if shown, otherwise null). "
        "Only include units available for immediate rent (not waitlisted, leased, or 'coming soon'). "
        "Return an empty list if no available units are found."
    )


    async def _scrape_with_llm(url: str) -> list[dict]:
        """
        Use Crawl4AI LLMExtractionStrategy to extract unit data from a building URL.

        Returns list of raw dicts (matching _UnitRecord schema).
        Returns empty list on extraction failure or no units found.
        """
        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise EnvironmentError(
                "ANTHROPIC_API_KEY is not set. "
                "Add it to your .env file or environment before running the LLM scraper."
            )

        strategy = LLMExtractionStrategy(
            llm_config=LLMConfig(
                provider=_HAIKU_PROVIDER,
                api_token=api_key,
            ),
            schema=_UnitRecord.model_json_schema(),
            extraction_type="schema",
            instruction=_EXTRACTION_INSTRUCTION,
        )
        config = CrawlerRunConfig(
            extraction_strategy=strategy,
            cache_mode=CacheMode.BYPASS,
        )

        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url, config=config)

        raw_content = getattr(result, "extracted_content", None) or ""
        try:
            parsed = json.loads(raw_content)
        except (json.JSONDecodeError, TypeError):
            # Malformed output from LLM — treat as empty (not a crash)
            return []

        if not isinstance(parsed, list):
            return []

        # Filter to dicts that have the minimum required fields
        units = []
        for item in parsed:
            if isinstance(item, dict) and item.get("unit_number") and item.get("bed_type") and item.get("rent"):
                units.append(item)

        return units


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability using LLM extraction (Crawl4AI + Claude Haiku).

        Works for any building URL — custom sites, Entrata buildings, and any
        platform that cannot be classified into a specific scraper.

        Returns list of raw unit dicts for normalize() / save_scrape_result().
        Returns empty list if LLM finds no available units.

        Raises EnvironmentError if ANTHROPIC_API_KEY is not set.
        """
        return asyncio.run(_scrape_with_llm(building.url))
    ```

    Create tests/test_scraper_llm.py — test the JSON parsing and output filtering logic (not the real Crawl4AI/LLM):

    - test_scrape_raises_environment_error_if_no_api_key: monkeypatch os.environ to remove ANTHROPIC_API_KEY → EnvironmentError
    - test_scrape_returns_empty_on_malformed_json: monkeypatch asyncio.run to return a result with extracted_content="not valid json" → []
    - test_scrape_returns_empty_on_null_content: extracted_content=None → []
    - test_scrape_returns_empty_on_non_list_json: extracted_content='{"key": "value"}' → []
    - test_scrape_filters_incomplete_records: 3-item JSON where one item is missing 'rent' → only 2 returned
    - test_scrape_returns_all_valid_records: valid JSON list with 2 full unit records → 2 dicts returned
    - test_unit_number_field_required: item without unit_number → filtered out
    - test_bed_type_field_required: item without bed_type → filtered out

    Use monkeypatch to replace asyncio.run in tests. Create a fake result object:
    ```python
    class FakeResult:
        extracted_content: str = ""

    async def fake_scrape(url):
        r = FakeResult()
        r.extracted_content = json.dumps([...])
        return r  # won't actually be used; monkeypatch replaces asyncio.run
    ```

    Simpler approach: monkeypatch _scrape_with_llm (the async function) by making
    scrape() call a module-level function that can be replaced:
    ```python
    # In tests, monkeypatch moxie.scrapers.tier3.llm._scrape_with_llm
    # to return a controlled list[dict] without real network calls
    ```
  </action>
  <verify>
    uv run pytest tests/test_scraper_llm.py -v (all tests pass)
    uv run pytest tests/ -v (full suite passes)
    uv run python -c "from moxie.scrapers.tier3.llm import scrape; print('import ok')"
  </verify>
  <done>LLM scraper importable. JSON parsing tested including malformed output. ANTHROPIC_API_KEY check raises clear error. Extraction instruction specific enough to return all required fields. All tests pass without real API calls.</done>
</task>

</tasks>

<verification>
- uv run pytest tests/test_scraper_llm.py -v
- uv run pytest tests/ -v (zero failures)
- uv run python -c "from moxie.scrapers.tier3.llm import scrape; print('import ok')"
- Confirm ANTHROPIC_API_KEY is listed in .env.example (create .env.example if it doesn't exist with: ANTHROPIC_API_KEY=your_key_here)
</verification>

<success_criteria>
- LLM scraper uses Crawl4AI LLMExtractionStrategy with Claude Haiku 3
- Malformed/null JSON returns empty list (not crash)
- Missing ANTHROPIC_API_KEY raises EnvironmentError with clear message
- Extraction instruction covers all UnitInput fields (unit_number, bed_type, rent, availability_date + optionals)
- Works for both custom sites and Entrata buildings (same interface)
- All tests pass without real API or network calls
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-08-SUMMARY.md`
</output>
