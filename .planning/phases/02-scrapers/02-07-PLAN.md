---
phase: 02-scrapers
plan: 07
type: execute
wave: 3
depends_on: [02-01]
files_modified:
  - src/moxie/scrapers/tier2/realpage.py
  - src/moxie/scrapers/tier2/groupfox.py
  - tests/test_scraper_realpage.py
  - tests/test_scraper_groupfox.py
autonomous: true
requirements: [SCRAP-05, SCRAP-07]

must_haves:
  truths:
    - "realpage.scrape(building) uses Crawl4AI AsyncWebCrawler to fetch and JS-render the building page, then BeautifulSoup to parse units"
    - "groupfox.scrape(building) constructs the /floorplans URL from the building URL, uses Crawl4AI to bypass bot detection, and parses available units"
    - "Both scrapers handle Crawl4AI failures (empty html result) by raising a platform-specific error"
    - "groupfox.scrape() fetches {building_url}/floorplans (or uses building.url directly if it already ends in /floorplans)"
  artifacts:
    - path: "src/moxie/scrapers/tier2/realpage.py"
      provides: "RealPage/G5 scraper using Crawl4AI + BeautifulSoup"
      exports: ["scrape"]
    - path: "src/moxie/scrapers/tier2/groupfox.py"
      provides: "Groupfox /floorplans scraper using Crawl4AI (bypasses 403)"
      exports: ["scrape"]
    - path: "tests/test_scraper_realpage.py"
      provides: "Tests for RealPage HTML parsing logic and error handling"
      min_lines: 25
    - path: "tests/test_scraper_groupfox.py"
      provides: "Tests for Groupfox URL construction, parsing, and name matching"
      min_lines: 30
  key_links:
    - from: "src/moxie/scrapers/tier2/realpage.py"
      to: "crawl4ai"
      via: "AsyncWebCrawler for JS rendering"
      pattern: "AsyncWebCrawler"
    - from: "src/moxie/scrapers/tier2/groupfox.py"
      to: "crawl4ai"
      via: "AsyncWebCrawler to bypass Groupfox 403 bot detection"
      pattern: "AsyncWebCrawler"
---

<objective>
Build Tier 2 Crawl4AI scrapers for RealPage/G5 (~10-15 buildings) and Groupfox (~12 buildings). Both require JS rendering or bot-detection bypass that httpx cannot provide. Both use Crawl4AI AsyncWebCrawler + BeautifulSoup.

Purpose: RealPage embeds unit data in JS-rendered widgets. Groupfox returns 403 to plain HTTP clients (confirmed in research). Both require a headless browser. Crawl4AI is the standard tool for this (per the research stack) and is already installed from 02-01.

Output: tier2/realpage.py, tier2/groupfox.py, tests for both (testing pure parse functions with static HTML; Crawl4AI calls are mocked/monkeypatched).
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-01-SUMMARY.md
@src/moxie/db/models.py
@src/moxie/scrapers/base.py
@src/moxie/normalizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: RealPage/G5 scraper (Crawl4AI + BeautifulSoup)</name>
  <files>
    src/moxie/scrapers/tier2/realpage.py
    tests/test_scraper_realpage.py
  </files>
  <action>
    Create src/moxie/scrapers/tier2/realpage.py:

    ```python
    """
    RealPage/G5 scraper — Tier 2 JS-rendered HTML.

    RealPage is the underlying platform for several management groups.
    G5 (g5searchmarketing.com) is RealPage's marketing CMS — unit data is
    typically embedded in JS-rendered widgets.

    Approach: Crawl4AI AsyncWebCrawler renders the page, BeautifulSoup parses the HTML.

    SELECTOR NOTE: RealPage listing structures vary by property configuration.
    Common patterns include data-unit attributes, .available-unit elements, and
    structured JSON-LD on the page. Selectors MUST be verified against real URLs.

    Platform: 'realpage'
    Coverage: ~10-15 buildings
    """
    import asyncio
    from bs4 import BeautifulSoup
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
    from moxie.db.models import Building


    class RealPageScraperError(RuntimeError):
        """Raised when Crawl4AI fails to render or returns empty HTML."""


    async def _fetch_rendered_html(url: str) -> str:
        """Use Crawl4AI to fetch and JS-render the RealPage listing page."""
        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url, config=config)
        return result.html or ""


    def _parse_html(html: str) -> list[dict]:
        """
        Parse unit data from rendered RealPage/G5 listing HTML.

        SELECTOR VERIFICATION REQUIRED: Verify against real realpage.com and
        g5searchmarketing.com property URLs.

        Common RealPage patterns:
        - .floorplan-item, .available-unit, [data-unit-type]
        - .unit-number in data-unit or text node
        - .unit-price, .unit-rent
        - .unit-beds, [data-beds]
        - .unit-availability, [data-available]
        """
        soup = BeautifulSoup(html, "html.parser")
        units = []

        for unit_el in soup.select(
            "[class*='available-unit'], [class*='floorplan-item'], [class*='unit-row']"
        ):
            bed_el = unit_el.select_one("[class*='bed'], [data-beds], [class*='bedroom']")
            rent_el = unit_el.select_one("[class*='price'], [class*='rent'], [data-price]")
            avail_el = unit_el.select_one("[class*='avail'], [class*='available'], [data-available]")
            num_el = unit_el.select_one("[class*='unit-number'], [data-unit], [class*='number']")

            if not (bed_el and rent_el):
                continue

            units.append({
                "unit_number": num_el.get_text(strip=True) if num_el else "N/A",
                "bed_type": bed_el.get_text(strip=True),
                "rent": rent_el.get_text(strip=True),
                "availability_date": avail_el.get_text(strip=True) if avail_el else "Available Now",
            })

        return units


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability from a RealPage/G5 listing page.

        Uses Crawl4AI for JS rendering. Raises RealPageScraperError if
        rendering fails or returns empty HTML.
        """
        html = asyncio.run(_fetch_rendered_html(building.url))
        if not html:
            raise RealPageScraperError(
                f"Crawl4AI returned empty HTML for RealPage building: {building.url}"
            )
        return _parse_html(html)
    ```

    Create tests/test_scraper_realpage.py:
    - test_parse_html_empty_returns_empty
    - test_parse_html_extracts_units: HTML fixture → list of dicts
    - test_scrape_raises_on_empty_html: monkeypatch _fetch_rendered_html to return "" → RealPageScraperError
    - test_scrape_returns_units_on_valid_html: monkeypatch _fetch_rendered_html to return SAMPLE_HTML → non-empty list

    Use monkeypatch to replace asyncio.run in tests — do not make real Crawl4AI calls in the test suite.
    Alternative: monkeypatch the _fetch_rendered_html coroutine directly.

    SAMPLE_HTML fixture:
    ```python
    SAMPLE_HTML = """
    <div class="available-unit">
      <span class="bedrooms">2 Bedrooms</span>
      <span class="unit-price">$2,800</span>
      <span class="unit-availability">April 1, 2026</span>
      <span class="unit-number">201</span>
    </div>
    """
    ```
  </action>
  <verify>
    uv run pytest tests/test_scraper_realpage.py -v
  </verify>
  <done>RealPage scraper uses Crawl4AI for rendering. Parse logic tested with static HTML. Empty HTML raises RealPageScraperError. All tests pass without real Crawl4AI calls.</done>
</task>

<task type="auto">
  <name>Task 2: Groupfox /floorplans scraper (Crawl4AI bot-bypass)</name>
  <files>
    src/moxie/scrapers/tier2/groupfox.py
    tests/test_scraper_groupfox.py
  </files>
  <action>
    Create src/moxie/scrapers/tier2/groupfox.py:

    Groupfox returns 403 to plain httpx (confirmed in research). Crawl4AI with full
    browser fingerprint bypasses this.

    URL pattern: Groupfox buildings follow `{subdomain}.groupfox.com/floorplans`.
    The building.url from Google Sheets may point to the root or the /floorplans page.
    Normalize to always use the /floorplans path.

    ```python
    """
    Groupfox scraper — Tier 2 JS-rendered HTML with bot-bypass.

    Groupfox returns HTTP 403 to non-browser HTTP clients (confirmed in research).
    Crawl4AI with Playwright browser fingerprint bypasses this detection.

    URL pattern: {subdomain}.groupfox.com/floorplans
    This scraper normalizes the building URL to always point to /floorplans.

    SELECTOR NOTE: Groupfox floorplan pages expose unit listings per floorplan category.
    URL paths like /floorplans/studio, /floorplans/one-bedroom may exist.
    The main /floorplans page typically lists all floorplans with unit counts.
    Selectors MUST be verified against a real Groupfox building URL.

    Platform: 'groupfox'
    Coverage: ~12 buildings
    """
    import asyncio
    from urllib.parse import urljoin, urlparse
    from bs4 import BeautifulSoup
    from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
    from moxie.db.models import Building


    class GroupfoxScraperError(RuntimeError):
        """Raised when Crawl4AI fails or returns empty HTML."""


    def _normalize_floorplans_url(building_url: str) -> str:
        """
        Ensure the URL points to the /floorplans path.
        If url already ends with /floorplans or /floorplans/, return as-is.
        Otherwise, append /floorplans to the base URL.
        """
        parsed = urlparse(building_url.rstrip("/"))
        path = parsed.path.rstrip("/")
        if path.endswith("/floorplans") or "/floorplans/" in path:
            return building_url
        # Construct floorplans URL: scheme://netloc/floorplans
        base = f"{parsed.scheme}://{parsed.netloc}"
        return f"{base}/floorplans"


    async def _fetch_rendered_html(url: str) -> str:
        """Use Crawl4AI (Playwright browser) to bypass Groupfox bot detection."""
        config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        async with AsyncWebCrawler() as crawler:
            result = await crawler.arun(url, config=config)
        return result.html or ""


    def _parse_html(html: str) -> list[dict]:
        """
        Parse unit data from Groupfox /floorplans page.

        SELECTOR VERIFICATION REQUIRED: Verify against a real Groupfox subdomain URL.

        Groupfox /floorplans patterns (approximate):
        - Floorplan cards: .floorplan-card, .floorplan-item
        - Bed count: .fp-beds, [data-beds], .bedrooms
        - Rent: .fp-rent, .price, [data-price]
        - Unit number: may be per-floorplan availability count, not individual unit numbers
        - Availability: 'Available Now', date, or 'Available [count] units'
        """
        soup = BeautifulSoup(html, "html.parser")
        units = []

        for fp_el in soup.select(
            "[class*='floorplan-card'], [class*='floorplan-item'], [class*='floor-plan']"
        ):
            bed_el = fp_el.select_one("[class*='bed'], [data-beds], [class*='bedroom']")
            rent_el = fp_el.select_one("[class*='rent'], [class*='price'], [data-price]")
            name_el = fp_el.select_one("[class*='fp-name'], [class*='floorplan-name'], h3, h4")
            avail_el = fp_el.select_one("[class*='avail'], [class*='available']")

            if not (bed_el and rent_el):
                continue

            # Groupfox may list floorplans rather than individual units;
            # use floorplan name as unit_number if no unit number is present
            fp_name = name_el.get_text(strip=True) if name_el else "N/A"
            units.append({
                "unit_number": fp_name,
                "floor_plan_name": fp_name,
                "bed_type": bed_el.get_text(strip=True),
                "rent": rent_el.get_text(strip=True),
                "availability_date": avail_el.get_text(strip=True) if avail_el else "Available Now",
            })

        return units


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability from a Groupfox /floorplans page.

        Normalizes URL to /floorplans, uses Crawl4AI to bypass bot detection,
        parses with BeautifulSoup.
        """
        floorplans_url = _normalize_floorplans_url(building.url)
        html = asyncio.run(_fetch_rendered_html(floorplans_url))
        if not html:
            raise GroupfoxScraperError(
                f"Crawl4AI returned empty HTML for Groupfox building: {floorplans_url}"
            )
        return _parse_html(html)
    ```

    Create tests/test_scraper_groupfox.py:
    - test_normalize_floorplans_url_already_has_path: "https://axis.groupfox.com/floorplans" → unchanged
    - test_normalize_floorplans_url_root: "https://axis.groupfox.com" → "https://axis.groupfox.com/floorplans"
    - test_normalize_floorplans_url_trailing_slash: "https://axis.groupfox.com/" → "https://axis.groupfox.com/floorplans"
    - test_normalize_floorplans_url_with_other_path: "https://axis.groupfox.com/about" → "https://axis.groupfox.com/floorplans"
    - test_parse_html_empty_returns_empty
    - test_parse_html_extracts_floorplans: HTML fixture with Groupfox-like markup → list of dicts
    - test_scrape_raises_on_empty_html: monkeypatch _fetch_rendered_html → GroupfoxScraperError
    - test_scrape_calls_normalized_url: monkeypatch _fetch_rendered_html, capture URL argument → confirm /floorplans is appended
  </action>
  <verify>
    uv run pytest tests/test_scraper_groupfox.py -v
    uv run pytest tests/ -v (full suite passes)
    uv run python -c "from moxie.scrapers.tier2.realpage import scrape; from moxie.scrapers.tier2.groupfox import scrape; print('imports ok')"
  </verify>
  <done>Both Crawl4AI-based scrapers importable and tested. Groupfox URL normalization confirmed. Empty HTML raises platform-specific errors. All tests pass without real browser calls.</done>
</task>

</tasks>

<verification>
- uv run pytest tests/test_scraper_realpage.py tests/test_scraper_groupfox.py -v
- uv run pytest tests/ -v (zero failures)
</verification>

<success_criteria>
- Both scrapers use Crawl4AI (AsyncWebCrawler) for rendering
- Groupfox URL normalization always targets /floorplans path
- Empty HTML from Crawl4AI raises platform-specific error (not silent empty list)
- All tests pass without real Crawl4AI/browser calls (monkeypatched)
- Selector comments document real-page verification requirement
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-07-SUMMARY.md`
</output>
