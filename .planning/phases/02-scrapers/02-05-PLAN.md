---
phase: 02-scrapers
plan: 05
type: execute
wave: 3
depends_on: [02-01]
files_modified:
  - src/moxie/scrapers/tier2/funnel.py
  - src/moxie/scrapers/tier2/appfolio.py
  - tests/test_scraper_funnel.py
  - tests/test_scraper_appfolio.py
autonomous: true
requirements: [SCRAP-04, SCRAP-08]

must_haves:
  truths:
    - "funnel.scrape(building) fetches the building's URL with httpx, parses unit data from HTML, and returns list[dict] for normalize()"
    - "appfolio.scrape(building) fetches the public AppFolio listing page and parses available units"
    - "Both scrapers handle HTTP errors (non-2xx) by raising an exception that save_scrape_result() interprets as failure"
    - "Unit parsing does not use regex on raw HTML — uses BeautifulSoup CSS selectors"
  artifacts:
    - path: "src/moxie/scrapers/tier2/funnel.py"
      provides: "Funnel/Nestio HTML scraper using httpx + BeautifulSoup"
      exports: ["scrape"]
    - path: "src/moxie/scrapers/tier2/appfolio.py"
      provides: "AppFolio public listing page scraper using httpx + BeautifulSoup"
      exports: ["scrape"]
    - path: "tests/test_scraper_funnel.py"
      provides: "Tests for Funnel HTML parsing using pytest-httpx or static HTML fixtures"
      min_lines: 30
    - path: "tests/test_scraper_appfolio.py"
      provides: "Tests for AppFolio HTML parsing using pytest-httpx or static HTML fixtures"
      min_lines: 25
  key_links:
    - from: "src/moxie/scrapers/tier2/funnel.py"
      to: "httpx"
      via: "httpx.get() for HTML fetch"
      pattern: "httpx\\.get"
    - from: "src/moxie/scrapers/tier2/appfolio.py"
      to: "httpx"
      via: "httpx.get() for HTML fetch"
      pattern: "httpx\\.get"
---

<objective>
Build Tier 2 HTML scrapers for Funnel/Nestio (~15-20 buildings) and AppFolio (~5-10 buildings). Both use httpx + BeautifulSoup (no JS rendering needed for initial implementation). Both return list[dict] for save_scrape_result().

Purpose: These are medium-volume platforms where the public listing pages expose unit data in static or server-rendered HTML. The Funnel API requires per-property keys (unavailable), so HTML scraping is the primary approach per RESEARCH.md.

Output: tier2/funnel.py, tier2/appfolio.py, tests for both using static HTML fixtures or pytest-httpx mocks.

NOTE ON IMPLEMENTATION: The exact HTML structure of Funnel and AppFolio listing pages is LOW confidence (not directly inspectable during research). The implementation should include a _parse_html(html) function that uses BeautifulSoup with reasonable selector heuristics and a clear comment noting that selectors must be verified against a real building URL during a manual spike. The scraper should still be runnable and testable against a static HTML fixture.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-01-SUMMARY.md
@src/moxie/db/models.py
@src/moxie/scrapers/base.py
@src/moxie/normalizer.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Funnel/Nestio HTML scraper</name>
  <files>
    src/moxie/scrapers/tier2/funnel.py
    tests/test_scraper_funnel.py
  </files>
  <action>
    Create src/moxie/scrapers/tier2/funnel.py:

    ```python
    """
    Funnel/Nestio scraper — Tier 2 HTML.

    Funnel's REST API (nestiolistings.com/api/v2/) requires per-property API keys
    which are not publicly available. This scraper fetches the public listing page HTML
    directly and parses unit data with BeautifulSoup.

    SELECTOR NOTE: CSS selectors in _parse_html() were written based on common Funnel
    listing page patterns. They MUST be verified against a real Funnel building URL
    before relying on the output. Run the scraper against a known nestiolistings.com
    or funnelleasing.com URL and inspect the output.

    Platform: 'funnel'
    Coverage: ~15-20 buildings
    """
    import httpx
    from bs4 import BeautifulSoup
    from moxie.db.models import Building

    _HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    }


    class FunnelScraperError(RuntimeError):
        """Raised on HTTP error or failed parse that signals scrape failure."""


    def _fetch_html(url: str) -> str:
        """Fetch the listing page HTML. Raises FunnelScraperError on non-2xx."""
        with httpx.Client(timeout=30.0, headers=_HEADERS, follow_redirects=True) as client:
            response = client.get(url)
        if response.status_code != 200:
            raise FunnelScraperError(
                f"Funnel listing page returned HTTP {response.status_code} for {url}"
            )
        return response.text


    def _parse_html(html: str) -> list[dict]:
        """
        Parse unit data from Funnel/Nestio listing page HTML.

        SELECTOR VERIFICATION REQUIRED: These selectors are heuristic based on
        common Funnel page structure. Verify against a real building URL.

        Expected Funnel HTML patterns (approximate):
        - Unit rows in elements with class containing 'unit', 'listing', or 'floorplan'
        - Bed type in element with class 'bedrooms' or data-attribute 'beds'
        - Rent in element with class 'price' or 'rent'
        - Availability in element with class 'available' or 'availability'
        - Unit number in element with class 'unit-number' or 'unit'
        """
        soup = BeautifulSoup(html, "html.parser")
        units = []

        # Strategy 1: Look for structured unit listing elements
        # Adjust selectors based on real page inspection
        for unit_el in soup.select("[class*='unit-listing'], [class*='unit-row'], [class*='floorplan-row']"):
            bed_el = unit_el.select_one("[class*='bed'], [class*='bedroom']")
            rent_el = unit_el.select_one("[class*='price'], [class*='rent']")
            avail_el = unit_el.select_one("[class*='avail'], [class*='available']")
            num_el = unit_el.select_one("[class*='unit-number'], [class*='number']")

            if not (bed_el and rent_el):
                continue  # skip incomplete rows

            unit_number = num_el.get_text(strip=True) if num_el else "N/A"
            rent_text = rent_el.get_text(strip=True)
            bed_text = bed_el.get_text(strip=True)
            avail_text = avail_el.get_text(strip=True) if avail_el else "Available Now"

            if not rent_text or not bed_text:
                continue

            units.append({
                "unit_number": unit_number,
                "bed_type": bed_text,
                "rent": rent_text,
                "availability_date": avail_text,
            })

        return units


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability from a Funnel/Nestio listing page.

        Returns list of raw unit dicts for normalize() / save_scrape_result().
        Raises FunnelScraperError on HTTP error (caller should pass scrape_succeeded=False
        to save_scrape_result).
        """
        html = _fetch_html(building.url)
        return _parse_html(html)
    ```

    Create tests/test_scraper_funnel.py using a minimal static HTML fixture:
    - test_parse_html_empty_returns_empty_list: "" → []
    - test_parse_html_no_units_returns_empty: HTML with no matching selectors → []
    - test_parse_html_extracts_units: HTML string with [class*='unit-listing'] elements → list of dicts
    - test_fetch_html_raises_on_non_200: use pytest-httpx to mock 404 response → FunnelScraperError
    - test_fetch_html_returns_html_on_200: mock 200 response → returns html string

    Provide a minimal HTML fixture string in the test file for parse tests:
    ```python
    SAMPLE_HTML = """
    <div class="unit-listing">
      <span class="bedrooms">1 Bed</span>
      <span class="price">$1,800/mo</span>
      <span class="availability">Available Now</span>
      <span class="unit-number">101</span>
    </div>
    <div class="unit-listing">
      <span class="bedrooms">Studio</span>
      <span class="price">$1,400/mo</span>
      <span class="availability">March 1, 2026</span>
      <span class="unit-number">202</span>
    </div>
    """
    ```
  </action>
  <verify>
    uv run pytest tests/test_scraper_funnel.py -v (all tests pass)
  </verify>
  <done>Funnel scraper parseable against static HTML fixture. HTTP error handling confirmed. CSS selectors documented as requiring real-page verification.</done>
</task>

<task type="auto">
  <name>Task 2: AppFolio public listing scraper</name>
  <files>
    src/moxie/scrapers/tier2/appfolio.py
    tests/test_scraper_appfolio.py
  </files>
  <action>
    Create src/moxie/scrapers/tier2/appfolio.py:

    ```python
    """
    AppFolio scraper — Tier 2 HTML.

    AppFolio's Stack API requires 50+ units and credentials. This scraper fetches
    the public AppFolio listing page (typically {subdomain}.appfolio.com/listings)
    and parses available unit data.

    SELECTOR NOTE: AppFolio listing pages typically render unit cards with structured
    data-attributes or class names. Selectors MUST be verified against a real AppFolio
    building URL before relying on output.

    Platform: 'appfolio'
    Coverage: ~5-10 buildings
    """
    import httpx
    from bs4 import BeautifulSoup
    from moxie.db.models import Building

    _HEADERS = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/120.0.0.0 Safari/537.36"
        ),
    }


    class AppFolioScraperError(RuntimeError):
        """Raised on HTTP error that signals scrape failure."""


    def _fetch_html(url: str) -> str:
        """Fetch the AppFolio listing page HTML. Raises AppFolioScraperError on non-2xx."""
        with httpx.Client(timeout=30.0, headers=_HEADERS, follow_redirects=True) as client:
            response = client.get(url)
        if response.status_code != 200:
            raise AppFolioScraperError(
                f"AppFolio listing page returned HTTP {response.status_code} for {url}"
            )
        return response.text


    def _parse_html(html: str) -> list[dict]:
        """
        Parse available units from AppFolio listing page HTML.

        SELECTOR VERIFICATION REQUIRED: Common AppFolio listing patterns include
        data attributes like data-unit-type, data-bedrooms, data-price, or
        class names like 'listing-item', 'unit-card', 'available-unit'.
        """
        soup = BeautifulSoup(html, "html.parser")
        units = []

        # Strategy: Try data-attribute selectors first, then class-based
        for unit_el in soup.select("[class*='listing-item'], [class*='unit-card'], [class*='available-unit']"):
            bed_el = unit_el.select_one("[class*='bedroom'], [class*='bed-count'], [data-bedrooms]")
            rent_el = unit_el.select_one("[class*='price'], [class*='rent'], [class*='rate']")
            avail_el = unit_el.select_one("[class*='avail'], [class*='move-in'], [class*='available']")
            num_el = unit_el.select_one("[class*='unit-number'], [class*='unit-name'], [class*='number']")

            if not (bed_el and rent_el):
                continue

            unit_number = (
                num_el.get("data-unit", num_el.get_text(strip=True))
                if num_el else "N/A"
            )
            units.append({
                "unit_number": unit_number,
                "bed_type": bed_el.get_text(strip=True),
                "rent": rent_el.get_text(strip=True),
                "availability_date": avail_el.get_text(strip=True) if avail_el else "Available Now",
            })

        return units


    def scrape(building: Building) -> list[dict]:
        """
        Scrape unit availability from an AppFolio public listing page.

        Returns list of raw unit dicts for normalize() / save_scrape_result().
        Raises AppFolioScraperError on HTTP error.
        """
        html = _fetch_html(building.url)
        return _parse_html(html)
    ```

    Create tests/test_scraper_appfolio.py with the same pattern as Funnel tests:
    - test_parse_html_empty_returns_empty_list
    - test_parse_html_extracts_units (using SAMPLE_HTML fixture with AppFolio-like markup)
    - test_fetch_html_raises_on_non_200 (pytest-httpx mock)
    - test_fetch_html_returns_content_on_200 (pytest-httpx mock)

    Sample HTML fixture:
    ```python
    SAMPLE_HTML = """
    <div class="listing-item">
      <span class="bedroom-count">2 Bedrooms</span>
      <span class="price">$2,500</span>
      <span class="available-date">April 1, 2026</span>
      <span class="unit-number">3B</span>
    </div>
    """
    ```
  </action>
  <verify>
    uv run pytest tests/test_scraper_appfolio.py -v (all tests pass)
    uv run pytest tests/ -v (full suite passes)
  </verify>
  <done>AppFolio scraper parseable against static HTML fixture. HTTP error handling confirmed. CSS selectors documented for real-page verification.</done>
</task>

</tasks>

<verification>
- uv run pytest tests/test_scraper_funnel.py tests/test_scraper_appfolio.py -v
- uv run pytest tests/ -v (full suite passes)
- uv run python -c "from moxie.scrapers.tier2.funnel import scrape; from moxie.scrapers.tier2.appfolio import scrape; print('imports ok')"
</verification>

<success_criteria>
- Both scrapers importable with correct scrape(building) signature
- HTTP error raises platform-specific exception (not silent empty list)
- HTML parsing uses BeautifulSoup (not regex)
- Tests pass without real network calls
- Selectors documented as requiring real-page verification
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-05-SUMMARY.md`
</output>
