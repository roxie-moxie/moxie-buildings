---
phase: 02-scrapers
plan: 09
type: execute
wave: 4
depends_on: [02-08]
files_modified:
  - scripts/llm_benchmark.py
  - .planning/phases/02-scrapers/02-LLM-BENCHMARK.md
autonomous: false
requirements: [SCRAP-09]

must_haves:
  truths:
    - "LLM scraper runs successfully against at least 5 real long-tail building URLs from the live database"
    - "Per-site token cost is estimated and documented (input tokens, output tokens, cost in USD) — benchmark acknowledges estimation methodology"
    - "Monthly projection at current Haiku pricing is within 20% of $120 (i.e. between $96 and $144) — OR confirmed to be significantly below"
    - "At least one Entrata building URL is included in the benchmark (validates Entrata → llm routing)"
    - "Benchmark results are documented in 02-LLM-BENCHMARK.md with cost table and monthly projection"
  artifacts:
    - path: "scripts/llm_benchmark.py"
      provides: "Benchmark script: run LLM scraper against N buildings, log token usage and cost"
      min_lines: 50
    - path: ".planning/phases/02-scrapers/02-LLM-BENCHMARK.md"
      provides: "Documented benchmark results: per-site tokens, cost, monthly projection"
      contains: "Monthly projection"
  key_links:
    - from: "scripts/llm_benchmark.py"
      to: "src/moxie/scrapers/tier3/llm.py"
      via: "scrape() call with real building objects from DB"
      pattern: "from moxie\\.scrapers\\.tier3\\.llm import scrape"
    - from: ".planning/phases/02-scrapers/02-LLM-BENCHMARK.md"
      to: "scripts/llm_benchmark.py"
      via: "Results produced by running the benchmark script"
      pattern: "llm_benchmark\\.py"

user_setup:
  - service: anthropic
    why: "LLM benchmark makes real Claude Haiku API calls — ANTHROPIC_API_KEY required"
    env_vars:
      - name: ANTHROPIC_API_KEY
        source: "Anthropic Console (console.anthropic.com) → API Keys"
    notes: "This benchmark will incur real API costs. Estimated cost: ~$0.05-$0.20 total for 5 sites. Ensure .env has ANTHROPIC_API_KEY set before running."
---

<objective>
Benchmark the LLM scraper against 5+ real long-tail building URLs to measure per-site token cost and confirm the monthly projection is within the $120 target. Document results in 02-LLM-BENCHMARK.md.

Purpose: Phase 2 success criterion 4 requires the LLM benchmarking to be completed before full-volume enablement. "Within 20% of $120" means $96-$144/month is acceptable; anything lower is better. Based on research, the actual cost should be $4.50-$36/month depending on model version.

Output: scripts/llm_benchmark.py (reusable benchmark tool), 02-LLM-BENCHMARK.md (results), human-verify checkpoint confirming cost is acceptable.
</objective>

<execution_context>
@C:/Users/eimil/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/eimil/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-scrapers/02-08-SUMMARY.md
@src/moxie/scrapers/tier3/llm.py
@src/moxie/db/session.py
@src/moxie/db/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Build benchmark script and run against 5 real sites</name>
  <files>
    scripts/llm_benchmark.py
  </files>
  <action>
    Create scripts/llm_benchmark.py:

    The script:
    1. Connects to the live moxie.db SQLite database
    2. Queries buildings with platform='llm' (the long-tail and Entrata buildings)
    3. Runs llm.scrape() against the first 5 (or N, configurable via --count arg) buildings
    4. Tracks token usage per site using Crawl4AI's result metadata or Anthropic's usage response
    5. Prints and saves a cost table

    Token cost tracking: Crawl4AI's LLMExtractionStrategy result may include token counts
    in result.extraction_metadata. If not available, estimate from the markdown length
    (1 token ≈ 4 characters) and from output length.

    Haiku 3 pricing: $0.25/MTok input, $1.25/MTok output.

    ```python
    #!/usr/bin/env python
    """
    LLM scraper benchmark — runs the Tier 3 LLM scraper against N real buildings
    with platform='llm' and measures per-site token cost.

    Usage:
        uv run python scripts/llm_benchmark.py [--count N]

    Requires:
        - ANTHROPIC_API_KEY in .env
        - Populated moxie.db with buildings synced from Google Sheets
        - crawl4ai-setup to have been run (Playwright browsers installed)
    """
    import argparse
    import sys
    import os
    import time
    from pathlib import Path

    # Ensure src/ is on the path when run via uv run python
    sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

    from dotenv import load_dotenv
    load_dotenv()

    from moxie.db.session import SessionLocal
    from moxie.db.models import Building

    # Haiku 3 pricing (USD per million tokens)
    HAIKU_INPUT_COST_PER_MTOK = 0.25
    HAIKU_OUTPUT_COST_PER_MTOK = 1.25


    def estimate_cost(input_tokens: int, output_tokens: int) -> float:
        """Return estimated cost in USD."""
        return (
            (input_tokens / 1_000_000) * HAIKU_INPUT_COST_PER_MTOK
            + (output_tokens / 1_000_000) * HAIKU_OUTPUT_COST_PER_MTOK
        )


    def run_benchmark(count: int = 5) -> None:
        db = SessionLocal()
        try:
            buildings = (
                db.query(Building)
                .filter(Building.platform == "llm", Building.url.isnot(None))
                .limit(count)
                .all()
            )
        finally:
            db.close()

        if not buildings:
            print("No buildings with platform='llm' found in DB. Run sheets-sync first.")
            sys.exit(1)

        print(f"Running LLM benchmark against {len(buildings)} buildings...")
        print("-" * 80)

        results = []
        total_input = 0
        total_output = 0
        total_cost = 0.0

        for i, building in enumerate(buildings, 1):
            print(f"[{i}/{len(buildings)}] {building.name} — {building.url}")
            start = time.time()

            # Import here to avoid loading crawl4ai before env is set
            from moxie.scrapers.tier3 import llm as llm_scraper
            try:
                units = llm_scraper.scrape(building)
                elapsed = time.time() - start

                # Token estimation: Crawl4AI may expose usage in future versions.
                # For now, estimate from output JSON length.
                # A more accurate approach is to instrument _scrape_with_llm to
                # return token counts from the Anthropic API response headers.
                output_json_chars = len(str(units))
                estimated_output_tokens = max(output_json_chars // 4, 10)
                # Input tokens: assume 10,000 avg (markdown from apartment page)
                estimated_input_tokens = 10_000

                cost = estimate_cost(estimated_input_tokens, estimated_output_tokens)
                total_input += estimated_input_tokens
                total_output += estimated_output_tokens
                total_cost += cost

                result = {
                    "building": building.name,
                    "url": building.url,
                    "units_found": len(units),
                    "elapsed_s": round(elapsed, 1),
                    "est_input_tokens": estimated_input_tokens,
                    "est_output_tokens": estimated_output_tokens,
                    "est_cost_usd": round(cost, 4),
                    "error": None,
                }
                print(f"  -> {len(units)} units found in {elapsed:.1f}s | est. ${cost:.4f}")
            except Exception as e:
                elapsed = time.time() - start
                result = {
                    "building": building.name,
                    "url": building.url,
                    "units_found": 0,
                    "elapsed_s": round(elapsed, 1),
                    "est_input_tokens": 0,
                    "est_output_tokens": 0,
                    "est_cost_usd": 0.0,
                    "error": str(e),
                }
                print(f"  -> ERROR in {elapsed:.1f}s: {e}")
            results.append(result)
            print()

        # Summary
        print("=" * 80)
        print(f"BENCHMARK SUMMARY ({len(results)} sites)")
        print(f"Total estimated cost: ${total_cost:.4f}")
        print(f"Average per site: ${total_cost / len(results):.4f}")
        buildings_per_day = 110  # estimated platform='llm' building count
        monthly_cost = (total_cost / len(results)) * buildings_per_day * 30
        print(f"Monthly projection ({buildings_per_day} buildings/day × 30 days): ${monthly_cost:.2f}")
        print(f"Target: <$120/month (within 20% = $96-$144)")
        print(f"Status: {'PASS' if monthly_cost <= 144 else 'EXCEEDS TARGET'}")
        print()

        # Save results
        output_path = Path(".planning/phases/02-scrapers/02-LLM-BENCHMARK.md")
        _write_benchmark_report(output_path, results, total_cost, monthly_cost, count)
        print(f"Results written to {output_path}")


    def _write_benchmark_report(path, results, total_cost, monthly_cost, count):
        lines = [
            "# LLM Scraper Benchmark Results",
            "",
            f"**Date:** {time.strftime('%Y-%m-%d')}",
            f"**Model:** claude-3-haiku-20240307",
            f"**Sites tested:** {count}",
            f"**Pricing:** $0.25/MTok input, $1.25/MTok output",
            "",
            "## Per-Site Results",
            "",
            "| Building | Units | Elapsed | Est. Input Tokens | Est. Output Tokens | Est. Cost |",
            "|----------|-------|---------|-------------------|--------------------|-----------|",
        ]
        for r in results:
            error_note = f" *(ERROR: {r['error'][:40]})*" if r["error"] else ""
            lines.append(
                f"| {r['building'][:40]} | {r['units_found']} | {r['elapsed_s']}s "
                f"| {r['est_input_tokens']:,} | {r['est_output_tokens']:,} "
                f"| ${r['est_cost_usd']:.4f}{error_note} |"
            )
        lines += [
            "",
            "## Cost Projection",
            "",
            f"| Metric | Value |",
            f"|--------|-------|",
            f"| Total cost ({count} sites) | ${total_cost:.4f} |",
            f"| Average per site | ${total_cost / count:.4f} |",
            f"| Est. daily (110 buildings) | ${(total_cost / count) * 110:.2f} |",
            f"| **Monthly projection** | **${monthly_cost:.2f}** |",
            f"| Target (<$120/month) | {'PASS' if monthly_cost <= 120 else 'EXCEEDS - review model or approach'} |",
            f"| Within 20% band ($96-$144) | {'PASS' if monthly_cost <= 144 else 'EXCEEDS BAND'} |",
            "",
            "## Notes",
            "",
            "- Token counts are estimated from output JSON length (~4 chars/token) and assumed 10K input tokens/page.",
            "- For precise token counts, instrument `_scrape_with_llm()` to capture Anthropic API usage headers.",
            "- If cost exceeds target, consider: Claude Batch API (50% discount), fewer sites per day, or caching.",
            "",
        ]
        path.write_text("\n".join(lines), encoding="utf-8")


    if __name__ == "__main__":
        parser = argparse.ArgumentParser(description="LLM scraper benchmark")
        parser.add_argument("--count", type=int, default=5, help="Number of sites to test (default: 5)")
        args = parser.parse_args()
        run_benchmark(args.count)
    ```

    Run the benchmark:
    ```
    uv run python scripts/llm_benchmark.py --count 5
    ```

    If the DB has fewer than 5 platform='llm' buildings (e.g. sheets-sync hasn't been run), run sheets-sync first:
    ```
    uv run sheets-sync
    ```
    Then re-run the benchmark. Select at least one building that appears to be an Entrata-hosted site (identifiable by looking at the URL patterns in the buildings table).
  </action>
  <verify>
    - scripts/llm_benchmark.py exists and runs without Python errors (even if API calls fail for individual sites)
    - .planning/phases/02-scrapers/02-LLM-BENCHMARK.md is created with results table
    - Monthly projection is calculated and shown
  </verify>
  <done>Benchmark script runs, results file written with per-site cost table and monthly projection calculation.</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Checkpoint: Human verify benchmark results and cost projection</name>
  <what-built>
    LLM scraper benchmark run against 5+ real building sites. Results documented in .planning/phases/02-scrapers/02-LLM-BENCHMARK.md including per-site token cost and monthly projection.
  </what-built>
  <how-to-verify>
    1. Open .planning/phases/02-scrapers/02-LLM-BENCHMARK.md
    2. Confirm it shows results for at least 5 sites
    3. Check the "Monthly projection" row — should be well under $120/month (research says $4.50-$36/month with Haiku 3)
    4. Check that at least one site returned actual unit records (units_found > 0) — confirms the LLM extraction is working
    5. If any sites errored, note the error pattern (bot detection, empty page, missing API key, etc.)
    6. Confirm the "Status" row shows PASS (within 20% of $120 = $96-$144 band)

    If monthly projection exceeds $144:
    - Consider switching to Claude Batch API (50% discount)
    - Or limit LLM tier to fewer buildings per day
    - Document the decision in STATE.md before proceeding

    If fewer than 5 sites have units_found > 0:
    - This may indicate bot detection, JS rendering issues, or extraction prompt problems
    - Check individual URLs manually in a browser
    - Adjust the extraction instruction in tier3/llm.py if needed and re-run
  </how-to-verify>
  <resume-signal>Type "approved" if cost is within target and at least 3 sites returned unit data. Or describe issues found.</resume-signal>
  <action>Human reviews .planning/phases/02-scrapers/02-LLM-BENCHMARK.md and confirms cost projection and extraction results.</action>
  <verify>02-LLM-BENCHMARK.md exists and monthly projection is documented</verify>
  <done>Human confirms cost is within target and extraction is working for at least 3 sites</done>
</task>

</tasks>

<verification>
- scripts/llm_benchmark.py runs without import errors: uv run python scripts/llm_benchmark.py --help
- .planning/phases/02-scrapers/02-LLM-BENCHMARK.md exists with results
- Human-verify checkpoint passed (cost within target, extraction working)
</verification>

<success_criteria>
- Benchmark run against at least 5 real platform='llm' buildings
- Per-site token cost documented
- Monthly projection calculated and shown in benchmark report
- Monthly projection is within 20% of $120 (i.e. ≤$144/month) — or lower
- At least 3 of 5 sites returned unit data (validates extraction is working)
- Human approval received confirming cost is acceptable
</success_criteria>

<output>
After completion, create `.planning/phases/02-scrapers/02-09-SUMMARY.md`
</output>
